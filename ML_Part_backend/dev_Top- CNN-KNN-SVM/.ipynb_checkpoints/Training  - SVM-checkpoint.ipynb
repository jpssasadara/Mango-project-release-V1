{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data=np.load('data.npy')\n",
    "target=np.load('target.npy')\n",
    "\n",
    "#loading the save numpy arrays in the previous code\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data,test_data,train_target,test_target=train_test_split(data,target,\n",
    "                                                              test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 36        \n",
      "=================================================================\n",
      "Total params: 316\n",
      "Trainable params: 316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 3 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "new_train_target=np_utils.to_categorical(train_target)\n",
    "print(new_train_target[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 68 samples, validate on 18 samples\n",
      "Epoch 1/1000\n",
      "68/68 [==============================] - 2s 29ms/step - loss: 25.2240 - accuracy: 0.2206 - val_loss: 21.2505 - val_accuracy: 0.2222\n",
      "Epoch 2/1000\n",
      "68/68 [==============================] - 0s 184us/step - loss: 20.2515 - accuracy: 0.2206 - val_loss: 16.5577 - val_accuracy: 0.2222\n",
      "Epoch 3/1000\n",
      "68/68 [==============================] - 0s 144us/step - loss: 15.9666 - accuracy: 0.1765 - val_loss: 12.2077 - val_accuracy: 0.2222\n",
      "Epoch 4/1000\n",
      "68/68 [==============================] - 0s 151us/step - loss: 11.9251 - accuracy: 0.1324 - val_loss: 8.1199 - val_accuracy: 0.1111\n",
      "Epoch 5/1000\n",
      "68/68 [==============================] - 0s 141us/step - loss: 8.3994 - accuracy: 0.2059 - val_loss: 4.8968 - val_accuracy: 0.2222\n",
      "Epoch 6/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 5.3048 - accuracy: 0.2500 - val_loss: 2.6596 - val_accuracy: 0.4444\n",
      "Epoch 7/1000\n",
      "68/68 [==============================] - 0s 128us/step - loss: 3.5779 - accuracy: 0.5147 - val_loss: 1.5008 - val_accuracy: 0.5556\n",
      "Epoch 8/1000\n",
      "68/68 [==============================] - 0s 153us/step - loss: 2.7060 - accuracy: 0.5882 - val_loss: 1.3193 - val_accuracy: 0.6111\n",
      "Epoch 9/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 2.9715 - accuracy: 0.5000 - val_loss: 1.6577 - val_accuracy: 0.5556\n",
      "Epoch 10/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 3.5008 - accuracy: 0.4412 - val_loss: 1.7470 - val_accuracy: 0.5556\n",
      "Epoch 11/1000\n",
      "68/68 [==============================] - 0s 141us/step - loss: 3.4740 - accuracy: 0.4412 - val_loss: 1.4406 - val_accuracy: 0.6667\n",
      "Epoch 12/1000\n",
      "68/68 [==============================] - 0s 148us/step - loss: 3.0218 - accuracy: 0.5147 - val_loss: 1.1274 - val_accuracy: 0.6111\n",
      "Epoch 13/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 2.5183 - accuracy: 0.5735 - val_loss: 1.0691 - val_accuracy: 0.6111\n",
      "Epoch 14/1000\n",
      "68/68 [==============================] - 0s 168us/step - loss: 2.2772 - accuracy: 0.5882 - val_loss: 1.2803 - val_accuracy: 0.6667\n",
      "Epoch 15/1000\n",
      "68/68 [==============================] - 0s 146us/step - loss: 2.3348 - accuracy: 0.5441 - val_loss: 1.4254 - val_accuracy: 0.6111\n",
      "Epoch 16/1000\n",
      "68/68 [==============================] - 0s 150us/step - loss: 2.3770 - accuracy: 0.5294 - val_loss: 1.3765 - val_accuracy: 0.6111\n",
      "Epoch 17/1000\n",
      "68/68 [==============================] - 0s 130us/step - loss: 2.2762 - accuracy: 0.5441 - val_loss: 1.2908 - val_accuracy: 0.5556\n",
      "Epoch 18/1000\n",
      "68/68 [==============================] - 0s 151us/step - loss: 2.1439 - accuracy: 0.5441 - val_loss: 1.1621 - val_accuracy: 0.5556\n",
      "Epoch 19/1000\n",
      "68/68 [==============================] - 0s 143us/step - loss: 2.0005 - accuracy: 0.5588 - val_loss: 1.0585 - val_accuracy: 0.5556\n",
      "Epoch 20/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 1.9313 - accuracy: 0.5441 - val_loss: 1.0214 - val_accuracy: 0.5556\n",
      "Epoch 21/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 1.8435 - accuracy: 0.5588 - val_loss: 0.9846 - val_accuracy: 0.5000\n",
      "Epoch 22/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 1.7764 - accuracy: 0.5588 - val_loss: 0.9516 - val_accuracy: 0.5556\n",
      "Epoch 23/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 1.6949 - accuracy: 0.5735 - val_loss: 0.9385 - val_accuracy: 0.6111\n",
      "Epoch 24/1000\n",
      "68/68 [==============================] - 0s 149us/step - loss: 1.6199 - accuracy: 0.5882 - val_loss: 0.9408 - val_accuracy: 0.6111\n",
      "Epoch 25/1000\n",
      "68/68 [==============================] - 0s 144us/step - loss: 1.5725 - accuracy: 0.5882 - val_loss: 0.9512 - val_accuracy: 0.5556\n",
      "Epoch 26/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 1.5141 - accuracy: 0.5588 - val_loss: 0.9390 - val_accuracy: 0.5556\n",
      "Epoch 27/1000\n",
      "68/68 [==============================] - 0s 129us/step - loss: 1.4544 - accuracy: 0.5588 - val_loss: 0.9155 - val_accuracy: 0.5556\n",
      "Epoch 28/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 1.3855 - accuracy: 0.5882 - val_loss: 0.8845 - val_accuracy: 0.5556\n",
      "Epoch 29/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 1.3053 - accuracy: 0.5882 - val_loss: 0.8485 - val_accuracy: 0.5556\n",
      "Epoch 30/1000\n",
      "68/68 [==============================] - 0s 143us/step - loss: 1.2460 - accuracy: 0.5882 - val_loss: 0.7987 - val_accuracy: 0.6667\n",
      "Epoch 31/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 1.1878 - accuracy: 0.6029 - val_loss: 0.7459 - val_accuracy: 0.7222\n",
      "Epoch 32/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 1.1195 - accuracy: 0.6324 - val_loss: 0.7180 - val_accuracy: 0.7222\n",
      "Epoch 33/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 1.0806 - accuracy: 0.6324 - val_loss: 0.7037 - val_accuracy: 0.7222\n",
      "Epoch 34/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 1.0381 - accuracy: 0.6176 - val_loss: 0.6955 - val_accuracy: 0.7222\n",
      "Epoch 35/1000\n",
      "68/68 [==============================] - 0s 177us/step - loss: 1.0044 - accuracy: 0.6176 - val_loss: 0.6784 - val_accuracy: 0.6667\n",
      "Epoch 36/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.9627 - accuracy: 0.6471 - val_loss: 0.6626 - val_accuracy: 0.7222\n",
      "Epoch 37/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.9316 - accuracy: 0.6765 - val_loss: 0.6489 - val_accuracy: 0.7222\n",
      "Epoch 38/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.9034 - accuracy: 0.6765 - val_loss: 0.6415 - val_accuracy: 0.7222\n",
      "Epoch 39/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.8708 - accuracy: 0.6618 - val_loss: 0.6566 - val_accuracy: 0.7222\n",
      "Epoch 40/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.8537 - accuracy: 0.6618 - val_loss: 0.6685 - val_accuracy: 0.6667\n",
      "Epoch 41/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.8402 - accuracy: 0.6471 - val_loss: 0.6695 - val_accuracy: 0.6667\n",
      "Epoch 42/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.8240 - accuracy: 0.6471 - val_loss: 0.6586 - val_accuracy: 0.6667\n",
      "Epoch 43/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.8062 - accuracy: 0.6618 - val_loss: 0.6377 - val_accuracy: 0.7222\n",
      "Epoch 44/1000\n",
      "68/68 [==============================] - 0s 206us/step - loss: 0.7799 - accuracy: 0.6765 - val_loss: 0.6005 - val_accuracy: 0.7222\n",
      "Epoch 45/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.7587 - accuracy: 0.6765 - val_loss: 0.5683 - val_accuracy: 0.7222\n",
      "Epoch 46/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.7443 - accuracy: 0.6471 - val_loss: 0.5594 - val_accuracy: 0.7222\n",
      "Epoch 47/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.7351 - accuracy: 0.6618 - val_loss: 0.5688 - val_accuracy: 0.7222\n",
      "Epoch 48/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.7220 - accuracy: 0.6765 - val_loss: 0.5772 - val_accuracy: 0.7222\n",
      "Epoch 49/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.7142 - accuracy: 0.6471 - val_loss: 0.5996 - val_accuracy: 0.7222\n",
      "Epoch 50/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.7084 - accuracy: 0.6765 - val_loss: 0.6274 - val_accuracy: 0.7222\n",
      "Epoch 51/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.7119 - accuracy: 0.6765 - val_loss: 0.6301 - val_accuracy: 0.7222\n",
      "Epoch 52/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.7074 - accuracy: 0.6912 - val_loss: 0.6146 - val_accuracy: 0.7222\n",
      "Epoch 53/1000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.6957 - accuracy: 0.71 - 0s 162us/step - loss: 0.7039 - accuracy: 0.6765 - val_loss: 0.6011 - val_accuracy: 0.7222\n",
      "Epoch 54/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.7048 - accuracy: 0.6765 - val_loss: 0.5984 - val_accuracy: 0.7222\n",
      "Epoch 55/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.7073 - accuracy: 0.6765 - val_loss: 0.5983 - val_accuracy: 0.7222\n",
      "Epoch 56/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.7079 - accuracy: 0.6471 - val_loss: 0.6004 - val_accuracy: 0.7222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.7057 - accuracy: 0.6618 - val_loss: 0.5951 - val_accuracy: 0.7222\n",
      "Epoch 58/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.7025 - accuracy: 0.6912 - val_loss: 0.5999 - val_accuracy: 0.7222\n",
      "Epoch 59/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.6970 - accuracy: 0.6765 - val_loss: 0.6169 - val_accuracy: 0.7222\n",
      "Epoch 60/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6993 - accuracy: 0.6912 - val_loss: 0.6411 - val_accuracy: 0.6667\n",
      "Epoch 61/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.7009 - accuracy: 0.6912 - val_loss: 0.6504 - val_accuracy: 0.6111\n",
      "Epoch 62/1000\n",
      "68/68 [==============================] - 0s 137us/step - loss: 0.7033 - accuracy: 0.6912 - val_loss: 0.6532 - val_accuracy: 0.6111\n",
      "Epoch 63/1000\n",
      "68/68 [==============================] - 0s 178us/step - loss: 0.7044 - accuracy: 0.6912 - val_loss: 0.6343 - val_accuracy: 0.7222\n",
      "Epoch 64/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6963 - accuracy: 0.6765 - val_loss: 0.5919 - val_accuracy: 0.7222\n",
      "Epoch 65/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.6993 - accuracy: 0.6471 - val_loss: 0.5784 - val_accuracy: 0.7222\n",
      "Epoch 66/1000\n",
      "68/68 [==============================] - 0s 119us/step - loss: 0.6928 - accuracy: 0.6471 - val_loss: 0.6089 - val_accuracy: 0.7222\n",
      "Epoch 67/1000\n",
      "68/68 [==============================] - 0s 220us/step - loss: 0.7027 - accuracy: 0.6618 - val_loss: 0.6455 - val_accuracy: 0.6667\n",
      "Epoch 68/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.6983 - accuracy: 0.6765 - val_loss: 0.6169 - val_accuracy: 0.7222\n",
      "Epoch 69/1000\n",
      "68/68 [==============================] - 0s 138us/step - loss: 0.6862 - accuracy: 0.6765 - val_loss: 0.5840 - val_accuracy: 0.7222\n",
      "Epoch 70/1000\n",
      "68/68 [==============================] - 0s 185us/step - loss: 0.6977 - accuracy: 0.6618 - val_loss: 0.5586 - val_accuracy: 0.7778\n",
      "Epoch 71/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.7092 - accuracy: 0.6618 - val_loss: 0.5644 - val_accuracy: 0.7778\n",
      "Epoch 72/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6977 - accuracy: 0.6618 - val_loss: 0.5961 - val_accuracy: 0.7222\n",
      "Epoch 73/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6883 - accuracy: 0.6912 - val_loss: 0.6204 - val_accuracy: 0.6667\n",
      "Epoch 74/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6888 - accuracy: 0.6912 - val_loss: 0.6181 - val_accuracy: 0.7222\n",
      "Epoch 75/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.6883 - accuracy: 0.6912 - val_loss: 0.6137 - val_accuracy: 0.7222\n",
      "Epoch 76/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.6835 - accuracy: 0.6912 - val_loss: 0.5965 - val_accuracy: 0.7222\n",
      "Epoch 77/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.6819 - accuracy: 0.6765 - val_loss: 0.5776 - val_accuracy: 0.7222\n",
      "Epoch 78/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6883 - accuracy: 0.6618 - val_loss: 0.5748 - val_accuracy: 0.7222\n",
      "Epoch 79/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6887 - accuracy: 0.6618 - val_loss: 0.5813 - val_accuracy: 0.7222\n",
      "Epoch 80/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.6869 - accuracy: 0.6471 - val_loss: 0.5945 - val_accuracy: 0.7222\n",
      "Epoch 81/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6831 - accuracy: 0.6618 - val_loss: 0.5895 - val_accuracy: 0.7222\n",
      "Epoch 82/1000\n",
      "68/68 [==============================] - 0s 131us/step - loss: 0.6842 - accuracy: 0.6618 - val_loss: 0.5811 - val_accuracy: 0.7222\n",
      "Epoch 83/1000\n",
      "68/68 [==============================] - 0s 173us/step - loss: 0.6859 - accuracy: 0.6765 - val_loss: 0.5877 - val_accuracy: 0.7222\n",
      "Epoch 84/1000\n",
      "68/68 [==============================] - 0s 159us/step - loss: 0.6814 - accuracy: 0.6618 - val_loss: 0.6000 - val_accuracy: 0.7222\n",
      "Epoch 85/1000\n",
      "68/68 [==============================] - 0s 153us/step - loss: 0.6785 - accuracy: 0.6471 - val_loss: 0.6222 - val_accuracy: 0.6667\n",
      "Epoch 86/1000\n",
      "68/68 [==============================] - 0s 85us/step - loss: 0.6894 - accuracy: 0.6765 - val_loss: 0.6463 - val_accuracy: 0.6667\n",
      "Epoch 87/1000\n",
      "68/68 [==============================] - 0s 180us/step - loss: 0.6896 - accuracy: 0.6912 - val_loss: 0.6151 - val_accuracy: 0.7222\n",
      "Epoch 88/1000\n",
      "68/68 [==============================] - 0s 269us/step - loss: 0.6818 - accuracy: 0.6765 - val_loss: 0.6066 - val_accuracy: 0.7222\n",
      "Epoch 89/1000\n",
      "68/68 [==============================] - 0s 157us/step - loss: 0.6778 - accuracy: 0.6912 - val_loss: 0.6245 - val_accuracy: 0.7222\n",
      "Epoch 90/1000\n",
      "68/68 [==============================] - 0s 165us/step - loss: 0.6831 - accuracy: 0.6912 - val_loss: 0.6107 - val_accuracy: 0.7222\n",
      "Epoch 91/1000\n",
      "68/68 [==============================] - 0s 150us/step - loss: 0.6762 - accuracy: 0.6912 - val_loss: 0.5858 - val_accuracy: 0.7222\n",
      "Epoch 92/1000\n",
      "68/68 [==============================] - 0s 161us/step - loss: 0.6738 - accuracy: 0.7059 - val_loss: 0.5531 - val_accuracy: 0.7778\n",
      "Epoch 93/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6891 - accuracy: 0.6471 - val_loss: 0.5451 - val_accuracy: 0.7778\n",
      "Epoch 94/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.7023 - accuracy: 0.6618 - val_loss: 0.5673 - val_accuracy: 0.7222\n",
      "Epoch 95/1000\n",
      "68/68 [==============================] - 0s 166us/step - loss: 0.6926 - accuracy: 0.6618 - val_loss: 0.6402 - val_accuracy: 0.6667\n",
      "Epoch 96/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.7036 - accuracy: 0.6471 - val_loss: 0.6864 - val_accuracy: 0.6667\n",
      "Epoch 97/1000\n",
      "68/68 [==============================] - 0s 163us/step - loss: 0.7156 - accuracy: 0.6471 - val_loss: 0.6821 - val_accuracy: 0.6667\n",
      "Epoch 98/1000\n",
      "68/68 [==============================] - 0s 177us/step - loss: 0.7052 - accuracy: 0.6618 - val_loss: 0.6392 - val_accuracy: 0.6667\n",
      "Epoch 99/1000\n",
      "68/68 [==============================] - 0s 197us/step - loss: 0.6809 - accuracy: 0.6618 - val_loss: 0.5925 - val_accuracy: 0.6667\n",
      "Epoch 100/1000\n",
      "68/68 [==============================] - 0s 167us/step - loss: 0.6789 - accuracy: 0.7059 - val_loss: 0.5497 - val_accuracy: 0.7778\n",
      "Epoch 101/1000\n",
      "68/68 [==============================] - 0s 167us/step - loss: 0.6854 - accuracy: 0.6324 - val_loss: 0.5427 - val_accuracy: 0.7778\n",
      "Epoch 102/1000\n",
      "68/68 [==============================] - 0s 193us/step - loss: 0.6913 - accuracy: 0.6324 - val_loss: 0.5441 - val_accuracy: 0.7778\n",
      "Epoch 103/1000\n",
      "68/68 [==============================] - 0s 184us/step - loss: 0.6867 - accuracy: 0.6618 - val_loss: 0.5475 - val_accuracy: 0.7778\n",
      "Epoch 104/1000\n",
      "68/68 [==============================] - 0s 170us/step - loss: 0.6789 - accuracy: 0.6471 - val_loss: 0.5604 - val_accuracy: 0.7222\n",
      "Epoch 105/1000\n",
      "68/68 [==============================] - 0s 419us/step - loss: 0.6717 - accuracy: 0.6471 - val_loss: 0.5864 - val_accuracy: 0.7222\n",
      "Epoch 106/1000\n",
      "68/68 [==============================] - 0s 189us/step - loss: 0.6659 - accuracy: 0.6912 - val_loss: 0.5986 - val_accuracy: 0.7222\n",
      "Epoch 107/1000\n",
      "68/68 [==============================] - 0s 391us/step - loss: 0.6663 - accuracy: 0.6912 - val_loss: 0.6008 - val_accuracy: 0.7222\n",
      "Epoch 108/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.6672 - accuracy: 0.6912 - val_loss: 0.5980 - val_accuracy: 0.7222\n",
      "Epoch 109/1000\n",
      "68/68 [==============================] - 0s 206us/step - loss: 0.6657 - accuracy: 0.6912 - val_loss: 0.5880 - val_accuracy: 0.7222\n",
      "Epoch 110/1000\n",
      "68/68 [==============================] - 0s 206us/step - loss: 0.6664 - accuracy: 0.6912 - val_loss: 0.6052 - val_accuracy: 0.7222\n",
      "Epoch 111/1000\n",
      "68/68 [==============================] - 0s 221us/step - loss: 0.6653 - accuracy: 0.6912 - val_loss: 0.6096 - val_accuracy: 0.7222\n",
      "Epoch 112/1000\n",
      "68/68 [==============================] - 0s 279us/step - loss: 0.6691 - accuracy: 0.6912 - val_loss: 0.6256 - val_accuracy: 0.6667\n",
      "Epoch 113/1000\n",
      "68/68 [==============================] - 0s 206us/step - loss: 0.6717 - accuracy: 0.6912 - val_loss: 0.6317 - val_accuracy: 0.6667\n",
      "Epoch 114/1000\n",
      "68/68 [==============================] - 0s 206us/step - loss: 0.6792 - accuracy: 0.6912 - val_loss: 0.6539 - val_accuracy: 0.6667\n",
      "Epoch 115/1000\n",
      "68/68 [==============================] - 0s 206us/step - loss: 0.6868 - accuracy: 0.6912 - val_loss: 0.6482 - val_accuracy: 0.6667\n",
      "Epoch 116/1000\n",
      "68/68 [==============================] - 0s 221us/step - loss: 0.6814 - accuracy: 0.6912 - val_loss: 0.6113 - val_accuracy: 0.6667\n",
      "Epoch 117/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.6684 - accuracy: 0.6471 - val_loss: 0.5763 - val_accuracy: 0.7222\n",
      "Epoch 118/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.6652 - accuracy: 0.6618 - val_loss: 0.5595 - val_accuracy: 0.7222\n",
      "Epoch 119/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.6631 - accuracy: 0.6618 - val_loss: 0.5653 - val_accuracy: 0.7222\n",
      "Epoch 120/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.6628 - accuracy: 0.6765 - val_loss: 0.5831 - val_accuracy: 0.7222\n",
      "Epoch 121/1000\n",
      "68/68 [==============================] - 0s 177us/step - loss: 0.6590 - accuracy: 0.6471 - val_loss: 0.6018 - val_accuracy: 0.7222\n",
      "Epoch 122/1000\n",
      "68/68 [==============================] - 0s 183us/step - loss: 0.6618 - accuracy: 0.6618 - val_loss: 0.6202 - val_accuracy: 0.6667\n",
      "Epoch 123/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.6674 - accuracy: 0.6912 - val_loss: 0.6190 - val_accuracy: 0.7222\n",
      "Epoch 124/1000\n",
      "68/68 [==============================] - 0s 177us/step - loss: 0.6562 - accuracy: 0.6912 - val_loss: 0.5631 - val_accuracy: 0.7222\n",
      "Epoch 125/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.6722 - accuracy: 0.6765 - val_loss: 0.5278 - val_accuracy: 0.7778\n",
      "Epoch 126/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.7061 - accuracy: 0.6324 - val_loss: 0.5280 - val_accuracy: 0.7778\n",
      "Epoch 127/1000\n",
      "68/68 [==============================] - 0s 250us/step - loss: 0.7156 - accuracy: 0.6176 - val_loss: 0.5372 - val_accuracy: 0.7778\n",
      "Epoch 128/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.6809 - accuracy: 0.6618 - val_loss: 0.5749 - val_accuracy: 0.7222\n",
      "Epoch 129/1000\n",
      "68/68 [==============================] - 0s 221us/step - loss: 0.6708 - accuracy: 0.6912 - val_loss: 0.6317 - val_accuracy: 0.6667\n",
      "Epoch 130/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.6686 - accuracy: 0.6618 - val_loss: 0.6393 - val_accuracy: 0.6667\n",
      "Epoch 131/1000\n",
      "68/68 [==============================] - 0s 175us/step - loss: 0.6711 - accuracy: 0.6618 - val_loss: 0.6437 - val_accuracy: 0.6667\n",
      "Epoch 132/1000\n",
      "68/68 [==============================] - 0s 237us/step - loss: 0.6695 - accuracy: 0.6912 - val_loss: 0.6137 - val_accuracy: 0.6667\n",
      "Epoch 133/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.6586 - accuracy: 0.6912 - val_loss: 0.5983 - val_accuracy: 0.7222\n",
      "Epoch 134/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.6543 - accuracy: 0.6912 - val_loss: 0.5863 - val_accuracy: 0.7222\n",
      "Epoch 135/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.6508 - accuracy: 0.6912 - val_loss: 0.5879 - val_accuracy: 0.7222\n",
      "Epoch 136/1000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.5872 - accuracy: 0.71 - 0s 191us/step - loss: 0.6515 - accuracy: 0.6912 - val_loss: 0.5868 - val_accuracy: 0.7222\n",
      "Epoch 137/1000\n",
      "68/68 [==============================] - 0s 177us/step - loss: 0.6491 - accuracy: 0.6912 - val_loss: 0.5640 - val_accuracy: 0.7222\n",
      "Epoch 138/1000\n",
      "68/68 [==============================] - 0s 250us/step - loss: 0.6554 - accuracy: 0.6765 - val_loss: 0.5420 - val_accuracy: 0.7778\n",
      "Epoch 139/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.6664 - accuracy: 0.6471 - val_loss: 0.5452 - val_accuracy: 0.7778\n",
      "Epoch 140/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.6569 - accuracy: 0.6618 - val_loss: 0.5658 - val_accuracy: 0.7222\n",
      "Epoch 141/1000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.6125 - accuracy: 0.71 - 0s 177us/step - loss: 0.6521 - accuracy: 0.6765 - val_loss: 0.5861 - val_accuracy: 0.7222\n",
      "Epoch 142/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.6484 - accuracy: 0.6618 - val_loss: 0.5755 - val_accuracy: 0.7222\n",
      "Epoch 143/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.6475 - accuracy: 0.6471 - val_loss: 0.5688 - val_accuracy: 0.7222\n",
      "Epoch 144/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.6556 - accuracy: 0.6618 - val_loss: 0.5717 - val_accuracy: 0.7222\n",
      "Epoch 145/1000\n",
      "68/68 [==============================] - 0s 177us/step - loss: 0.6617 - accuracy: 0.6765 - val_loss: 0.5787 - val_accuracy: 0.7222\n",
      "Epoch 146/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.6551 - accuracy: 0.6618 - val_loss: 0.6061 - val_accuracy: 0.7222\n",
      "Epoch 147/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.6583 - accuracy: 0.6765 - val_loss: 0.6455 - val_accuracy: 0.6111\n",
      "Epoch 148/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.6659 - accuracy: 0.6912 - val_loss: 0.6369 - val_accuracy: 0.6667\n",
      "Epoch 149/1000\n",
      "68/68 [==============================] - 0s 279us/step - loss: 0.6569 - accuracy: 0.6912 - val_loss: 0.5929 - val_accuracy: 0.7222\n",
      "Epoch 150/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.6479 - accuracy: 0.7059 - val_loss: 0.5597 - val_accuracy: 0.7778\n",
      "Epoch 151/1000\n",
      "68/68 [==============================] - 0s 221us/step - loss: 0.6571 - accuracy: 0.6912 - val_loss: 0.5342 - val_accuracy: 0.7778\n",
      "Epoch 152/1000\n",
      "68/68 [==============================] - 0s 235us/step - loss: 0.7087 - accuracy: 0.6176 - val_loss: 0.5305 - val_accuracy: 0.7778\n",
      "Epoch 153/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.6707 - accuracy: 0.6471 - val_loss: 0.5764 - val_accuracy: 0.7222\n",
      "Epoch 154/1000\n",
      "68/68 [==============================] - 0s 206us/step - loss: 0.6360 - accuracy: 0.6912 - val_loss: 0.6705 - val_accuracy: 0.6111\n",
      "Epoch 155/1000\n",
      "68/68 [==============================] - 0s 165us/step - loss: 0.6787 - accuracy: 0.6912 - val_loss: 0.7403 - val_accuracy: 0.6111\n",
      "Epoch 156/1000\n",
      "68/68 [==============================] - 0s 210us/step - loss: 0.7250 - accuracy: 0.6618 - val_loss: 0.7147 - val_accuracy: 0.6111\n",
      "Epoch 157/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6839 - accuracy: 0.6765 - val_loss: 0.6056 - val_accuracy: 0.7222\n",
      "Epoch 158/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6435 - accuracy: 0.6618 - val_loss: 0.5340 - val_accuracy: 0.8333\n",
      "Epoch 159/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.6629 - accuracy: 0.6765 - val_loss: 0.5150 - val_accuracy: 0.7778\n",
      "Epoch 160/1000\n",
      "68/68 [==============================] - 0s 148us/step - loss: 0.7129 - accuracy: 0.6029 - val_loss: 0.5130 - val_accuracy: 0.7778\n",
      "Epoch 161/1000\n",
      "68/68 [==============================] - 0s 194us/step - loss: 0.6761 - accuracy: 0.6618 - val_loss: 0.5556 - val_accuracy: 0.7222\n",
      "Epoch 162/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6466 - accuracy: 0.6618 - val_loss: 0.6360 - val_accuracy: 0.6667\n",
      "Epoch 163/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.6643 - accuracy: 0.6912 - val_loss: 0.7001 - val_accuracy: 0.6111\n",
      "Epoch 164/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.7041 - accuracy: 0.6471 - val_loss: 0.7203 - val_accuracy: 0.6111\n",
      "Epoch 165/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.7033 - accuracy: 0.6618 - val_loss: 0.6453 - val_accuracy: 0.6111\n",
      "Epoch 166/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6518 - accuracy: 0.7059 - val_loss: 0.5809 - val_accuracy: 0.7222\n",
      "Epoch 167/1000\n",
      "68/68 [==============================] - 0s 87us/step - loss: 0.6462 - accuracy: 0.6765 - val_loss: 0.5391 - val_accuracy: 0.7778\n",
      "Epoch 168/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 233us/step - loss: 0.6849 - accuracy: 0.6176 - val_loss: 0.5345 - val_accuracy: 0.7778\n",
      "Epoch 169/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6900 - accuracy: 0.6471 - val_loss: 0.5547 - val_accuracy: 0.7778\n",
      "Epoch 170/1000\n",
      "68/68 [==============================] - 0s 138us/step - loss: 0.6585 - accuracy: 0.6765 - val_loss: 0.5917 - val_accuracy: 0.7222\n",
      "Epoch 171/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6514 - accuracy: 0.6912 - val_loss: 0.6287 - val_accuracy: 0.6667\n",
      "Epoch 172/1000\n",
      "68/68 [==============================] - 0s 128us/step - loss: 0.6564 - accuracy: 0.6618 - val_loss: 0.6355 - val_accuracy: 0.6667\n",
      "Epoch 173/1000\n",
      "68/68 [==============================] - 0s 174us/step - loss: 0.6553 - accuracy: 0.6765 - val_loss: 0.6161 - val_accuracy: 0.6667\n",
      "Epoch 174/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.6480 - accuracy: 0.6765 - val_loss: 0.6151 - val_accuracy: 0.6667\n",
      "Epoch 175/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.6469 - accuracy: 0.6912 - val_loss: 0.6110 - val_accuracy: 0.6667\n",
      "Epoch 176/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.6407 - accuracy: 0.6912 - val_loss: 0.5776 - val_accuracy: 0.7222\n",
      "Epoch 177/1000\n",
      "68/68 [==============================] - 0s 192us/step - loss: 0.6356 - accuracy: 0.6912 - val_loss: 0.5423 - val_accuracy: 0.7222\n",
      "Epoch 178/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.6367 - accuracy: 0.6765 - val_loss: 0.5366 - val_accuracy: 0.7222\n",
      "Epoch 179/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.6365 - accuracy: 0.6765 - val_loss: 0.5529 - val_accuracy: 0.7222\n",
      "Epoch 180/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6384 - accuracy: 0.6618 - val_loss: 0.5737 - val_accuracy: 0.7222\n",
      "Epoch 181/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.6345 - accuracy: 0.6765 - val_loss: 0.5604 - val_accuracy: 0.7222\n",
      "Epoch 182/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.6334 - accuracy: 0.6912 - val_loss: 0.5401 - val_accuracy: 0.7222\n",
      "Epoch 183/1000\n",
      "68/68 [==============================] - 0s 169us/step - loss: 0.6345 - accuracy: 0.6618 - val_loss: 0.5315 - val_accuracy: 0.7222\n",
      "Epoch 184/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.6376 - accuracy: 0.6765 - val_loss: 0.5268 - val_accuracy: 0.8333\n",
      "Epoch 185/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6467 - accuracy: 0.6765 - val_loss: 0.5289 - val_accuracy: 0.7778\n",
      "Epoch 186/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6409 - accuracy: 0.6912 - val_loss: 0.5572 - val_accuracy: 0.7222\n",
      "Epoch 187/1000\n",
      "68/68 [==============================] - 0s 153us/step - loss: 0.6286 - accuracy: 0.6765 - val_loss: 0.5961 - val_accuracy: 0.7222\n",
      "Epoch 188/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.6361 - accuracy: 0.7059 - val_loss: 0.6747 - val_accuracy: 0.6111\n",
      "Epoch 189/1000\n",
      "68/68 [==============================] - 0s 177us/step - loss: 0.6900 - accuracy: 0.6765 - val_loss: 0.7164 - val_accuracy: 0.6111\n",
      "Epoch 190/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6946 - accuracy: 0.6765 - val_loss: 0.6665 - val_accuracy: 0.6667\n",
      "Epoch 191/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6584 - accuracy: 0.6618 - val_loss: 0.5847 - val_accuracy: 0.6667\n",
      "Epoch 192/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.6488 - accuracy: 0.7059 - val_loss: 0.5470 - val_accuracy: 0.7778\n",
      "Epoch 193/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6394 - accuracy: 0.7059 - val_loss: 0.5491 - val_accuracy: 0.7778\n",
      "Epoch 194/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6303 - accuracy: 0.7059 - val_loss: 0.5668 - val_accuracy: 0.7222\n",
      "Epoch 195/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.6271 - accuracy: 0.7059 - val_loss: 0.5768 - val_accuracy: 0.7222\n",
      "Epoch 196/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6292 - accuracy: 0.6912 - val_loss: 0.5669 - val_accuracy: 0.7222\n",
      "Epoch 197/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6275 - accuracy: 0.6471 - val_loss: 0.5334 - val_accuracy: 0.7222\n",
      "Epoch 198/1000\n",
      "68/68 [==============================] - 0s 141us/step - loss: 0.6347 - accuracy: 0.6912 - val_loss: 0.5133 - val_accuracy: 0.8333\n",
      "Epoch 199/1000\n",
      "68/68 [==============================] - 0s 106us/step - loss: 0.6403 - accuracy: 0.6765 - val_loss: 0.5284 - val_accuracy: 0.7222\n",
      "Epoch 200/1000\n",
      "68/68 [==============================] - 0s 244us/step - loss: 0.6253 - accuracy: 0.6765 - val_loss: 0.5669 - val_accuracy: 0.7222\n",
      "Epoch 201/1000\n",
      "68/68 [==============================] - 0s 168us/step - loss: 0.6343 - accuracy: 0.6765 - val_loss: 0.6135 - val_accuracy: 0.6667\n",
      "Epoch 202/1000\n",
      "68/68 [==============================] - 0s 180us/step - loss: 0.6464 - accuracy: 0.6765 - val_loss: 0.6141 - val_accuracy: 0.6667\n",
      "Epoch 203/1000\n",
      "68/68 [==============================] - 0s 140us/step - loss: 0.6361 - accuracy: 0.6912 - val_loss: 0.5623 - val_accuracy: 0.7222\n",
      "Epoch 204/1000\n",
      "68/68 [==============================] - 0s 134us/step - loss: 0.6321 - accuracy: 0.6471 - val_loss: 0.5102 - val_accuracy: 0.8333\n",
      "Epoch 205/1000\n",
      "68/68 [==============================] - 0s 144us/step - loss: 0.6789 - accuracy: 0.6618 - val_loss: 0.4995 - val_accuracy: 0.7778\n",
      "Epoch 206/1000\n",
      "68/68 [==============================] - 0s 158us/step - loss: 0.6944 - accuracy: 0.6471 - val_loss: 0.5113 - val_accuracy: 0.7778\n",
      "Epoch 207/1000\n",
      "68/68 [==============================] - 0s 178us/step - loss: 0.6451 - accuracy: 0.6618 - val_loss: 0.6157 - val_accuracy: 0.6111\n",
      "Epoch 208/1000\n",
      "68/68 [==============================] - 0s 175us/step - loss: 0.6530 - accuracy: 0.7059 - val_loss: 0.7143 - val_accuracy: 0.6111\n",
      "Epoch 209/1000\n",
      "68/68 [==============================] - 0s 155us/step - loss: 0.7005 - accuracy: 0.6618 - val_loss: 0.7059 - val_accuracy: 0.6111\n",
      "Epoch 210/1000\n",
      "68/68 [==============================] - 0s 158us/step - loss: 0.6891 - accuracy: 0.6765 - val_loss: 0.6504 - val_accuracy: 0.6667\n",
      "Epoch 211/1000\n",
      "68/68 [==============================] - 0s 165us/step - loss: 0.6580 - accuracy: 0.6765 - val_loss: 0.5885 - val_accuracy: 0.6667\n",
      "Epoch 212/1000\n",
      "68/68 [==============================] - 0s 154us/step - loss: 0.6293 - accuracy: 0.6912 - val_loss: 0.5375 - val_accuracy: 0.7222\n",
      "Epoch 213/1000\n",
      "68/68 [==============================] - 0s 173us/step - loss: 0.6216 - accuracy: 0.6912 - val_loss: 0.5063 - val_accuracy: 0.8333\n",
      "Epoch 214/1000\n",
      "68/68 [==============================] - 0s 155us/step - loss: 0.6391 - accuracy: 0.6765 - val_loss: 0.5157 - val_accuracy: 0.7778\n",
      "Epoch 215/1000\n",
      "68/68 [==============================] - 0s 164us/step - loss: 0.6211 - accuracy: 0.6912 - val_loss: 0.5675 - val_accuracy: 0.7222\n",
      "Epoch 216/1000\n",
      "68/68 [==============================] - 0s 131us/step - loss: 0.6359 - accuracy: 0.6912 - val_loss: 0.5908 - val_accuracy: 0.6667\n",
      "Epoch 217/1000\n",
      "68/68 [==============================] - 0s 190us/step - loss: 0.6264 - accuracy: 0.6912 - val_loss: 0.5482 - val_accuracy: 0.7222\n",
      "Epoch 218/1000\n",
      "68/68 [==============================] - 0s 190us/step - loss: 0.6163 - accuracy: 0.6765 - val_loss: 0.5224 - val_accuracy: 0.7778\n",
      "Epoch 219/1000\n",
      "68/68 [==============================] - 0s 243us/step - loss: 0.6254 - accuracy: 0.6765 - val_loss: 0.5089 - val_accuracy: 0.8333\n",
      "Epoch 220/1000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.6595 - accuracy: 0.62 - 0s 163us/step - loss: 0.6331 - accuracy: 0.6765 - val_loss: 0.5174 - val_accuracy: 0.8333\n",
      "Epoch 221/1000\n",
      "68/68 [==============================] - 0s 172us/step - loss: 0.6242 - accuracy: 0.6765 - val_loss: 0.5631 - val_accuracy: 0.7222\n",
      "Epoch 222/1000\n",
      "68/68 [==============================] - 0s 101us/step - loss: 0.6251 - accuracy: 0.6765 - val_loss: 0.6132 - val_accuracy: 0.6667\n",
      "Epoch 223/1000\n",
      "68/68 [==============================] - 0s 260us/step - loss: 0.6482 - accuracy: 0.6765 - val_loss: 0.6180 - val_accuracy: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224/1000\n",
      "68/68 [==============================] - 0s 166us/step - loss: 0.6398 - accuracy: 0.6765 - val_loss: 0.5642 - val_accuracy: 0.7222\n",
      "Epoch 225/1000\n",
      "68/68 [==============================] - 0s 167us/step - loss: 0.6195 - accuracy: 0.6618 - val_loss: 0.5296 - val_accuracy: 0.7222\n",
      "Epoch 226/1000\n",
      "68/68 [==============================] - 0s 175us/step - loss: 0.6225 - accuracy: 0.6765 - val_loss: 0.5282 - val_accuracy: 0.7778\n",
      "Epoch 227/1000\n",
      "68/68 [==============================] - 0s 159us/step - loss: 0.6236 - accuracy: 0.6765 - val_loss: 0.5641 - val_accuracy: 0.7222\n",
      "Epoch 228/1000\n",
      "68/68 [==============================] - 0s 237us/step - loss: 0.6201 - accuracy: 0.6765 - val_loss: 0.5923 - val_accuracy: 0.6667\n",
      "Epoch 229/1000\n",
      "68/68 [==============================] - 0s 136us/step - loss: 0.6267 - accuracy: 0.6912 - val_loss: 0.5928 - val_accuracy: 0.6667\n",
      "Epoch 230/1000\n",
      "68/68 [==============================] - 0s 183us/step - loss: 0.6230 - accuracy: 0.6912 - val_loss: 0.5591 - val_accuracy: 0.7222\n",
      "Epoch 231/1000\n",
      "68/68 [==============================] - 0s 170us/step - loss: 0.6146 - accuracy: 0.6765 - val_loss: 0.5252 - val_accuracy: 0.7778\n",
      "Epoch 232/1000\n",
      "68/68 [==============================] - 0s 179us/step - loss: 0.6190 - accuracy: 0.6765 - val_loss: 0.5139 - val_accuracy: 0.7778\n",
      "Epoch 233/1000\n",
      "68/68 [==============================] - 0s 159us/step - loss: 0.6281 - accuracy: 0.6765 - val_loss: 0.5196 - val_accuracy: 0.7778\n",
      "Epoch 234/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.6223 - accuracy: 0.6912 - val_loss: 0.5323 - val_accuracy: 0.7778\n",
      "Epoch 235/1000\n",
      "68/68 [==============================] - 0s 111us/step - loss: 0.6166 - accuracy: 0.6765 - val_loss: 0.5267 - val_accuracy: 0.7778\n",
      "Epoch 236/1000\n",
      "68/68 [==============================] - 0s 264us/step - loss: 0.6191 - accuracy: 0.6765 - val_loss: 0.5143 - val_accuracy: 0.7778\n",
      "Epoch 237/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.6300 - accuracy: 0.6765 - val_loss: 0.5173 - val_accuracy: 0.7778\n",
      "Epoch 238/1000\n",
      "68/68 [==============================] - 0s 124us/step - loss: 0.6270 - accuracy: 0.6765 - val_loss: 0.5597 - val_accuracy: 0.7222\n",
      "Epoch 239/1000\n",
      "68/68 [==============================] - 0s 127us/step - loss: 0.6171 - accuracy: 0.6765 - val_loss: 0.5984 - val_accuracy: 0.6667\n",
      "Epoch 240/1000\n",
      "68/68 [==============================] - 0s 412us/step - loss: 0.6366 - accuracy: 0.6912 - val_loss: 0.6060 - val_accuracy: 0.6667\n",
      "Epoch 241/1000\n",
      "68/68 [==============================] - 0s 230us/step - loss: 0.6276 - accuracy: 0.7059 - val_loss: 0.5479 - val_accuracy: 0.7222\n",
      "Epoch 242/1000\n",
      "68/68 [==============================] - 0s 230us/step - loss: 0.6090 - accuracy: 0.6912 - val_loss: 0.5029 - val_accuracy: 0.8333\n",
      "Epoch 243/1000\n",
      "68/68 [==============================] - 0s 230us/step - loss: 0.6398 - accuracy: 0.6765 - val_loss: 0.4881 - val_accuracy: 0.7778\n",
      "Epoch 244/1000\n",
      "68/68 [==============================] - 0s 230us/step - loss: 0.6823 - accuracy: 0.6471 - val_loss: 0.4918 - val_accuracy: 0.7222\n",
      "Epoch 245/1000\n",
      "68/68 [==============================] - 0s 265us/step - loss: 0.6783 - accuracy: 0.6471 - val_loss: 0.5044 - val_accuracy: 0.7778\n",
      "Epoch 246/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6290 - accuracy: 0.6765 - val_loss: 0.5594 - val_accuracy: 0.7222\n",
      "Epoch 247/1000\n",
      "68/68 [==============================] - 0s 323us/step - loss: 0.6261 - accuracy: 0.6765 - val_loss: 0.6407 - val_accuracy: 0.6667\n",
      "Epoch 248/1000\n",
      "68/68 [==============================] - 0s 134us/step - loss: 0.6629 - accuracy: 0.6912 - val_loss: 0.6737 - val_accuracy: 0.6111\n",
      "Epoch 249/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6777 - accuracy: 0.6765 - val_loss: 0.6546 - val_accuracy: 0.6111\n",
      "Epoch 250/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6515 - accuracy: 0.7059 - val_loss: 0.5855 - val_accuracy: 0.6667\n",
      "Epoch 251/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6164 - accuracy: 0.6912 - val_loss: 0.5216 - val_accuracy: 0.7778\n",
      "Epoch 252/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6241 - accuracy: 0.6765 - val_loss: 0.4951 - val_accuracy: 0.7778\n",
      "Epoch 253/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6385 - accuracy: 0.6471 - val_loss: 0.4950 - val_accuracy: 0.7778\n",
      "Epoch 254/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6340 - accuracy: 0.6471 - val_loss: 0.5151 - val_accuracy: 0.7778\n",
      "Epoch 255/1000\n",
      "68/68 [==============================] - 0s 171us/step - loss: 0.6121 - accuracy: 0.6912 - val_loss: 0.5444 - val_accuracy: 0.7222\n",
      "Epoch 256/1000\n",
      "68/68 [==============================] - 0s 173us/step - loss: 0.6104 - accuracy: 0.6912 - val_loss: 0.5809 - val_accuracy: 0.7222\n",
      "Epoch 257/1000\n",
      "68/68 [==============================] - 0s 164us/step - loss: 0.6230 - accuracy: 0.7059 - val_loss: 0.5880 - val_accuracy: 0.7222\n",
      "Epoch 258/1000\n",
      "68/68 [==============================] - 0s 160us/step - loss: 0.6173 - accuracy: 0.6912 - val_loss: 0.5470 - val_accuracy: 0.7222\n",
      "Epoch 259/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6097 - accuracy: 0.6765 - val_loss: 0.5294 - val_accuracy: 0.7222\n",
      "Epoch 260/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.6109 - accuracy: 0.6765 - val_loss: 0.5213 - val_accuracy: 0.7778\n",
      "Epoch 261/1000\n",
      "68/68 [==============================] - 0s 143us/step - loss: 0.6146 - accuracy: 0.6912 - val_loss: 0.5226 - val_accuracy: 0.7778\n",
      "Epoch 262/1000\n",
      "68/68 [==============================] - 0s 160us/step - loss: 0.6131 - accuracy: 0.6765 - val_loss: 0.5313 - val_accuracy: 0.7778\n",
      "Epoch 263/1000\n",
      "68/68 [==============================] - 0s 0us/step - loss: 0.6124 - accuracy: 0.7059 - val_loss: 0.5453 - val_accuracy: 0.7222\n",
      "Epoch 264/1000\n",
      "68/68 [==============================] - 0s 376us/step - loss: 0.6166 - accuracy: 0.6765 - val_loss: 0.5861 - val_accuracy: 0.6667\n",
      "Epoch 265/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6330 - accuracy: 0.6618 - val_loss: 0.6125 - val_accuracy: 0.6667\n",
      "Epoch 266/1000\n",
      "68/68 [==============================] - 0s 145us/step - loss: 0.6379 - accuracy: 0.6618 - val_loss: 0.5883 - val_accuracy: 0.6667\n",
      "Epoch 267/1000\n",
      "68/68 [==============================] - 0s 138us/step - loss: 0.6244 - accuracy: 0.6618 - val_loss: 0.5596 - val_accuracy: 0.7222\n",
      "Epoch 268/1000\n",
      "68/68 [==============================] - 0s 184us/step - loss: 0.6175 - accuracy: 0.6765 - val_loss: 0.5347 - val_accuracy: 0.7778\n",
      "Epoch 269/1000\n",
      "68/68 [==============================] - 0s 101us/step - loss: 0.6183 - accuracy: 0.7059 - val_loss: 0.5298 - val_accuracy: 0.7778\n",
      "Epoch 270/1000\n",
      "68/68 [==============================] - 0s 259us/step - loss: 0.6116 - accuracy: 0.6912 - val_loss: 0.5431 - val_accuracy: 0.7222\n",
      "Epoch 271/1000\n",
      "68/68 [==============================] - 0s 179us/step - loss: 0.6083 - accuracy: 0.6765 - val_loss: 0.5398 - val_accuracy: 0.7222\n",
      "Epoch 272/1000\n",
      "68/68 [==============================] - 0s 153us/step - loss: 0.6073 - accuracy: 0.6912 - val_loss: 0.5334 - val_accuracy: 0.7778\n",
      "Epoch 273/1000\n",
      "68/68 [==============================] - 0s 152us/step - loss: 0.6084 - accuracy: 0.6912 - val_loss: 0.5267 - val_accuracy: 0.7778\n",
      "Epoch 274/1000\n",
      "68/68 [==============================] - 0s 168us/step - loss: 0.6077 - accuracy: 0.6912 - val_loss: 0.5196 - val_accuracy: 0.7778\n",
      "Epoch 275/1000\n",
      "68/68 [==============================] - 0s 125us/step - loss: 0.6095 - accuracy: 0.6765 - val_loss: 0.5173 - val_accuracy: 0.7778\n",
      "Epoch 276/1000\n",
      "68/68 [==============================] - 0s 248us/step - loss: 0.6090 - accuracy: 0.6765 - val_loss: 0.5201 - val_accuracy: 0.7778\n",
      "Epoch 277/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6071 - accuracy: 0.6765 - val_loss: 0.5281 - val_accuracy: 0.7778\n",
      "Epoch 278/1000\n",
      "68/68 [==============================] - 0s 145us/step - loss: 0.6090 - accuracy: 0.6912 - val_loss: 0.5371 - val_accuracy: 0.7778\n",
      "Epoch 279/1000\n",
      "68/68 [==============================] - 0s 89us/step - loss: 0.6088 - accuracy: 0.6912 - val_loss: 0.5631 - val_accuracy: 0.6667\n",
      "Epoch 280/1000\n",
      "68/68 [==============================] - 0s 223us/step - loss: 0.6152 - accuracy: 0.7206 - val_loss: 0.5773 - val_accuracy: 0.6667\n",
      "Epoch 281/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.6179 - accuracy: 0.7206 - val_loss: 0.5614 - val_accuracy: 0.6667\n",
      "Epoch 282/1000\n",
      "68/68 [==============================] - 0s 177us/step - loss: 0.6146 - accuracy: 0.7059 - val_loss: 0.5332 - val_accuracy: 0.7778\n",
      "Epoch 283/1000\n",
      "68/68 [==============================] - 0s 169us/step - loss: 0.6115 - accuracy: 0.6912 - val_loss: 0.5181 - val_accuracy: 0.7778\n",
      "Epoch 284/1000\n",
      "68/68 [==============================] - 0s 194us/step - loss: 0.6128 - accuracy: 0.6912 - val_loss: 0.5200 - val_accuracy: 0.7778\n",
      "Epoch 285/1000\n",
      "68/68 [==============================] - 0s 173us/step - loss: 0.6092 - accuracy: 0.6912 - val_loss: 0.5257 - val_accuracy: 0.7778\n",
      "Epoch 286/1000\n",
      "68/68 [==============================] - 0s 174us/step - loss: 0.6095 - accuracy: 0.6912 - val_loss: 0.5622 - val_accuracy: 0.7222\n",
      "Epoch 287/1000\n",
      "68/68 [==============================] - 0s 177us/step - loss: 0.6085 - accuracy: 0.7059 - val_loss: 0.5830 - val_accuracy: 0.7222\n",
      "Epoch 288/1000\n",
      "68/68 [==============================] - 0s 172us/step - loss: 0.6150 - accuracy: 0.7059 - val_loss: 0.5845 - val_accuracy: 0.7222\n",
      "Epoch 289/1000\n",
      "68/68 [==============================] - 0s 155us/step - loss: 0.6138 - accuracy: 0.7059 - val_loss: 0.5660 - val_accuracy: 0.7222\n",
      "Epoch 290/1000\n",
      "68/68 [==============================] - 0s 175us/step - loss: 0.6064 - accuracy: 0.6912 - val_loss: 0.5413 - val_accuracy: 0.7222\n",
      "Epoch 291/1000\n",
      "68/68 [==============================] - 0s 161us/step - loss: 0.6045 - accuracy: 0.7059 - val_loss: 0.5341 - val_accuracy: 0.7222\n",
      "Epoch 292/1000\n",
      "68/68 [==============================] - 0s 152us/step - loss: 0.6028 - accuracy: 0.6912 - val_loss: 0.5679 - val_accuracy: 0.7222\n",
      "Epoch 293/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.6088 - accuracy: 0.6912 - val_loss: 0.5846 - val_accuracy: 0.6667\n",
      "Epoch 294/1000\n",
      "68/68 [==============================] - 0s 146us/step - loss: 0.6136 - accuracy: 0.6765 - val_loss: 0.5710 - val_accuracy: 0.7222\n",
      "Epoch 295/1000\n",
      "68/68 [==============================] - 0s 156us/step - loss: 0.6082 - accuracy: 0.6912 - val_loss: 0.5464 - val_accuracy: 0.7222\n",
      "Epoch 296/1000\n",
      "68/68 [==============================] - 0s 180us/step - loss: 0.6070 - accuracy: 0.6912 - val_loss: 0.5246 - val_accuracy: 0.7778\n",
      "Epoch 297/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.6074 - accuracy: 0.6912 - val_loss: 0.5243 - val_accuracy: 0.7778\n",
      "Epoch 298/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.6029 - accuracy: 0.6765 - val_loss: 0.5302 - val_accuracy: 0.7778\n",
      "Epoch 299/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.6011 - accuracy: 0.6912 - val_loss: 0.5424 - val_accuracy: 0.7222\n",
      "Epoch 300/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.6007 - accuracy: 0.7059 - val_loss: 0.5437 - val_accuracy: 0.7222\n",
      "Epoch 301/1000\n",
      "68/68 [==============================] - 0s 117us/step - loss: 0.6043 - accuracy: 0.7206 - val_loss: 0.5344 - val_accuracy: 0.7778\n",
      "Epoch 302/1000\n",
      "68/68 [==============================] - 0s 137us/step - loss: 0.6030 - accuracy: 0.6912 - val_loss: 0.5085 - val_accuracy: 0.7778\n",
      "Epoch 303/1000\n",
      "68/68 [==============================] - 0s 152us/step - loss: 0.6060 - accuracy: 0.6912 - val_loss: 0.5124 - val_accuracy: 0.7778\n",
      "Epoch 304/1000\n",
      "68/68 [==============================] - 0s 152us/step - loss: 0.6037 - accuracy: 0.6912 - val_loss: 0.5303 - val_accuracy: 0.7778\n",
      "Epoch 305/1000\n",
      "68/68 [==============================] - 0s 133us/step - loss: 0.5972 - accuracy: 0.7059 - val_loss: 0.5541 - val_accuracy: 0.7222\n",
      "Epoch 306/1000\n",
      "68/68 [==============================] - 0s 146us/step - loss: 0.5996 - accuracy: 0.7059 - val_loss: 0.5809 - val_accuracy: 0.7222\n",
      "Epoch 307/1000\n",
      "68/68 [==============================] - 0s 157us/step - loss: 0.6128 - accuracy: 0.6765 - val_loss: 0.5905 - val_accuracy: 0.7222\n",
      "Epoch 308/1000\n",
      "68/68 [==============================] - 0s 158us/step - loss: 0.6121 - accuracy: 0.6912 - val_loss: 0.5652 - val_accuracy: 0.7222\n",
      "Epoch 309/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6096 - accuracy: 0.7059 - val_loss: 0.5585 - val_accuracy: 0.7222\n",
      "Epoch 310/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6135 - accuracy: 0.7059 - val_loss: 0.5597 - val_accuracy: 0.7222\n",
      "Epoch 311/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.6157 - accuracy: 0.7059 - val_loss: 0.5573 - val_accuracy: 0.7222\n",
      "Epoch 312/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.6092 - accuracy: 0.7059 - val_loss: 0.5330 - val_accuracy: 0.7778\n",
      "Epoch 313/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.6112 - accuracy: 0.6765 - val_loss: 0.5100 - val_accuracy: 0.7778\n",
      "Epoch 314/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.6114 - accuracy: 0.6912 - val_loss: 0.5011 - val_accuracy: 0.7778\n",
      "Epoch 315/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6084 - accuracy: 0.6912 - val_loss: 0.4930 - val_accuracy: 0.7778\n",
      "Epoch 316/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.6118 - accuracy: 0.6765 - val_loss: 0.4867 - val_accuracy: 0.8333\n",
      "Epoch 317/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.6133 - accuracy: 0.6765 - val_loss: 0.4840 - val_accuracy: 0.8333\n",
      "Epoch 318/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6160 - accuracy: 0.6912 - val_loss: 0.4787 - val_accuracy: 0.8333\n",
      "Epoch 319/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6195 - accuracy: 0.6618 - val_loss: 0.4834 - val_accuracy: 0.8333\n",
      "Epoch 320/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6119 - accuracy: 0.6912 - val_loss: 0.5072 - val_accuracy: 0.7778\n",
      "Epoch 321/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5965 - accuracy: 0.7059 - val_loss: 0.5575 - val_accuracy: 0.7222\n",
      "Epoch 322/1000\n",
      "68/68 [==============================] - 0s 133us/step - loss: 0.6105 - accuracy: 0.6912 - val_loss: 0.5927 - val_accuracy: 0.6111\n",
      "Epoch 323/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6176 - accuracy: 0.6765 - val_loss: 0.5774 - val_accuracy: 0.7222\n",
      "Epoch 324/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.6077 - accuracy: 0.6912 - val_loss: 0.5297 - val_accuracy: 0.7222\n",
      "Epoch 325/1000\n",
      "68/68 [==============================] - 0s 161us/step - loss: 0.6009 - accuracy: 0.6912 - val_loss: 0.4828 - val_accuracy: 0.8333\n",
      "Epoch 326/1000\n",
      "68/68 [==============================] - 0s 138us/step - loss: 0.6293 - accuracy: 0.6618 - val_loss: 0.4782 - val_accuracy: 0.7778\n",
      "Epoch 327/1000\n",
      "68/68 [==============================] - 0s 142us/step - loss: 0.6529 - accuracy: 0.6618 - val_loss: 0.4852 - val_accuracy: 0.7778\n",
      "Epoch 328/1000\n",
      "68/68 [==============================] - 0s 85us/step - loss: 0.6231 - accuracy: 0.6765 - val_loss: 0.5192 - val_accuracy: 0.7778\n",
      "Epoch 329/1000\n",
      "68/68 [==============================] - 0s 263us/step - loss: 0.5927 - accuracy: 0.7206 - val_loss: 0.5897 - val_accuracy: 0.6667\n",
      "Epoch 330/1000\n",
      "68/68 [==============================] - 0s 174us/step - loss: 0.6161 - accuracy: 0.6912 - val_loss: 0.6379 - val_accuracy: 0.6111\n",
      "Epoch 331/1000\n",
      "68/68 [==============================] - 0s 0us/step - loss: 0.6301 - accuracy: 0.6912 - val_loss: 0.6030 - val_accuracy: 0.6667\n",
      "Epoch 332/1000\n",
      "68/68 [==============================] - 0s 144us/step - loss: 0.6039 - accuracy: 0.7059 - val_loss: 0.5421 - val_accuracy: 0.7778\n",
      "Epoch 333/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.5954 - accuracy: 0.7059 - val_loss: 0.5057 - val_accuracy: 0.7778\n",
      "Epoch 334/1000\n",
      "68/68 [==============================] - 0s 240us/step - loss: 0.6184 - accuracy: 0.6765 - val_loss: 0.4948 - val_accuracy: 0.8333\n",
      "Epoch 335/1000\n",
      "68/68 [==============================] - 0s 163us/step - loss: 0.6439 - accuracy: 0.6765 - val_loss: 0.4913 - val_accuracy: 0.8333\n",
      "Epoch 336/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 165us/step - loss: 0.6213 - accuracy: 0.6471 - val_loss: 0.5185 - val_accuracy: 0.7778\n",
      "Epoch 337/1000\n",
      "68/68 [==============================] - 0s 145us/step - loss: 0.5982 - accuracy: 0.7206 - val_loss: 0.5511 - val_accuracy: 0.7222\n",
      "Epoch 338/1000\n",
      "68/68 [==============================] - 0s 168us/step - loss: 0.5992 - accuracy: 0.6912 - val_loss: 0.5600 - val_accuracy: 0.7222\n",
      "Epoch 339/1000\n",
      "68/68 [==============================] - 0s 196us/step - loss: 0.6049 - accuracy: 0.6912 - val_loss: 0.5470 - val_accuracy: 0.7222\n",
      "Epoch 340/1000\n",
      "68/68 [==============================] - 0s 161us/step - loss: 0.6069 - accuracy: 0.6765 - val_loss: 0.5095 - val_accuracy: 0.7778\n",
      "Epoch 341/1000\n",
      "68/68 [==============================] - 0s 156us/step - loss: 0.5992 - accuracy: 0.7059 - val_loss: 0.4988 - val_accuracy: 0.8333\n",
      "Epoch 342/1000\n",
      "68/68 [==============================] - 0s 172us/step - loss: 0.6068 - accuracy: 0.6912 - val_loss: 0.4947 - val_accuracy: 0.8333\n",
      "Epoch 343/1000\n",
      "68/68 [==============================] - 0s 183us/step - loss: 0.6066 - accuracy: 0.7059 - val_loss: 0.5256 - val_accuracy: 0.7222\n",
      "Epoch 344/1000\n",
      "68/68 [==============================] - 0s 211us/step - loss: 0.6009 - accuracy: 0.7059 - val_loss: 0.5394 - val_accuracy: 0.7222\n",
      "Epoch 345/1000\n",
      "68/68 [==============================] - 0s 137us/step - loss: 0.5982 - accuracy: 0.7059 - val_loss: 0.5539 - val_accuracy: 0.7222\n",
      "Epoch 346/1000\n",
      "68/68 [==============================] - 0s 141us/step - loss: 0.6014 - accuracy: 0.7059 - val_loss: 0.5544 - val_accuracy: 0.7222\n",
      "Epoch 347/1000\n",
      "68/68 [==============================] - 0s 165us/step - loss: 0.6005 - accuracy: 0.7206 - val_loss: 0.5511 - val_accuracy: 0.6667\n",
      "Epoch 348/1000\n",
      "68/68 [==============================] - 0s 165us/step - loss: 0.5993 - accuracy: 0.7206 - val_loss: 0.5393 - val_accuracy: 0.7222\n",
      "Epoch 349/1000\n",
      "68/68 [==============================] - 0s 154us/step - loss: 0.5948 - accuracy: 0.7206 - val_loss: 0.5238 - val_accuracy: 0.7778\n",
      "Epoch 350/1000\n",
      "68/68 [==============================] - 0s 153us/step - loss: 0.5959 - accuracy: 0.6912 - val_loss: 0.5170 - val_accuracy: 0.7778\n",
      "Epoch 351/1000\n",
      "68/68 [==============================] - 0s 175us/step - loss: 0.5965 - accuracy: 0.6912 - val_loss: 0.5209 - val_accuracy: 0.7778\n",
      "Epoch 352/1000\n",
      "68/68 [==============================] - 0s 107us/step - loss: 0.5957 - accuracy: 0.6912 - val_loss: 0.5076 - val_accuracy: 0.7778\n",
      "Epoch 353/1000\n",
      "68/68 [==============================] - 0s 231us/step - loss: 0.5999 - accuracy: 0.6912 - val_loss: 0.5075 - val_accuracy: 0.7778\n",
      "Epoch 354/1000\n",
      "68/68 [==============================] - 0s 134us/step - loss: 0.5972 - accuracy: 0.6912 - val_loss: 0.5232 - val_accuracy: 0.7778\n",
      "Epoch 355/1000\n",
      "68/68 [==============================] - 0s 168us/step - loss: 0.5943 - accuracy: 0.7059 - val_loss: 0.5535 - val_accuracy: 0.7222\n",
      "Epoch 356/1000\n",
      "68/68 [==============================] - 0s 158us/step - loss: 0.5945 - accuracy: 0.7059 - val_loss: 0.5586 - val_accuracy: 0.7222\n",
      "Epoch 357/1000\n",
      "68/68 [==============================] - 0s 158us/step - loss: 0.5978 - accuracy: 0.7059 - val_loss: 0.5405 - val_accuracy: 0.7222\n",
      "Epoch 358/1000\n",
      "68/68 [==============================] - 0s 153us/step - loss: 0.5907 - accuracy: 0.7059 - val_loss: 0.5037 - val_accuracy: 0.8333\n",
      "Epoch 359/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.6172 - accuracy: 0.6765 - val_loss: 0.4866 - val_accuracy: 0.7778\n",
      "Epoch 360/1000\n",
      "68/68 [==============================] - 0s 164us/step - loss: 0.6249 - accuracy: 0.6618 - val_loss: 0.4876 - val_accuracy: 0.8333\n",
      "Epoch 361/1000\n",
      "68/68 [==============================] - 0s 175us/step - loss: 0.6049 - accuracy: 0.6765 - val_loss: 0.5108 - val_accuracy: 0.7778\n",
      "Epoch 362/1000\n",
      "68/68 [==============================] - 0s 155us/step - loss: 0.5879 - accuracy: 0.7206 - val_loss: 0.5677 - val_accuracy: 0.7222\n",
      "Epoch 363/1000\n",
      "68/68 [==============================] - 0s 155us/step - loss: 0.6043 - accuracy: 0.6765 - val_loss: 0.6285 - val_accuracy: 0.6111\n",
      "Epoch 364/1000\n",
      "68/68 [==============================] - 0s 161us/step - loss: 0.6273 - accuracy: 0.6912 - val_loss: 0.6378 - val_accuracy: 0.6111\n",
      "Epoch 365/1000\n",
      "68/68 [==============================] - 0s 179us/step - loss: 0.6305 - accuracy: 0.6765 - val_loss: 0.5872 - val_accuracy: 0.6667\n",
      "Epoch 366/1000\n",
      "68/68 [==============================] - 0s 189us/step - loss: 0.6018 - accuracy: 0.7206 - val_loss: 0.5035 - val_accuracy: 0.7778\n",
      "Epoch 367/1000\n",
      "68/68 [==============================] - 0s 180us/step - loss: 0.5977 - accuracy: 0.6912 - val_loss: 0.4736 - val_accuracy: 0.7778\n",
      "Epoch 368/1000\n",
      "68/68 [==============================] - 0s 181us/step - loss: 0.6295 - accuracy: 0.6765 - val_loss: 0.4732 - val_accuracy: 0.7778\n",
      "Epoch 369/1000\n",
      "68/68 [==============================] - 0s 168us/step - loss: 0.6126 - accuracy: 0.6618 - val_loss: 0.4948 - val_accuracy: 0.7778\n",
      "Epoch 370/1000\n",
      "68/68 [==============================] - 0s 173us/step - loss: 0.5944 - accuracy: 0.6912 - val_loss: 0.5253 - val_accuracy: 0.7778\n",
      "Epoch 371/1000\n",
      "68/68 [==============================] - 0s 157us/step - loss: 0.5916 - accuracy: 0.7059 - val_loss: 0.5361 - val_accuracy: 0.7222\n",
      "Epoch 372/1000\n",
      "68/68 [==============================] - 0s 173us/step - loss: 0.5939 - accuracy: 0.7059 - val_loss: 0.5302 - val_accuracy: 0.7778\n",
      "Epoch 373/1000\n",
      "68/68 [==============================] - 0s 154us/step - loss: 0.5915 - accuracy: 0.7059 - val_loss: 0.5082 - val_accuracy: 0.7778\n",
      "Epoch 374/1000\n",
      "68/68 [==============================] - 0s 123us/step - loss: 0.5945 - accuracy: 0.6912 - val_loss: 0.4940 - val_accuracy: 0.7778\n",
      "Epoch 375/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.5972 - accuracy: 0.6912 - val_loss: 0.4998 - val_accuracy: 0.7778\n",
      "Epoch 376/1000\n",
      "68/68 [==============================] - 0s 170us/step - loss: 0.5922 - accuracy: 0.7206 - val_loss: 0.5389 - val_accuracy: 0.7222\n",
      "Epoch 377/1000\n",
      "68/68 [==============================] - 0s 92us/step - loss: 0.5907 - accuracy: 0.7059 - val_loss: 0.5576 - val_accuracy: 0.7222\n",
      "Epoch 378/1000\n",
      "68/68 [==============================] - 0s 251us/step - loss: 0.5928 - accuracy: 0.7059 - val_loss: 0.5387 - val_accuracy: 0.7222\n",
      "Epoch 379/1000\n",
      "68/68 [==============================] - 0s 155us/step - loss: 0.5867 - accuracy: 0.7059 - val_loss: 0.5197 - val_accuracy: 0.7778\n",
      "Epoch 380/1000\n",
      "68/68 [==============================] - 0s 153us/step - loss: 0.5870 - accuracy: 0.7206 - val_loss: 0.5117 - val_accuracy: 0.7778\n",
      "Epoch 381/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.5865 - accuracy: 0.7059 - val_loss: 0.5094 - val_accuracy: 0.7778\n",
      "Epoch 382/1000\n",
      "68/68 [==============================] - 0s 156us/step - loss: 0.5873 - accuracy: 0.6912 - val_loss: 0.4946 - val_accuracy: 0.7778\n",
      "Epoch 383/1000\n",
      "68/68 [==============================] - 0s 111us/step - loss: 0.5998 - accuracy: 0.6765 - val_loss: 0.4855 - val_accuracy: 0.7778\n",
      "Epoch 384/1000\n",
      "68/68 [==============================] - 0s 240us/step - loss: 0.5926 - accuracy: 0.6765 - val_loss: 0.5145 - val_accuracy: 0.7778\n",
      "Epoch 385/1000\n",
      "68/68 [==============================] - 0s 0us/step - loss: 0.5845 - accuracy: 0.7206 - val_loss: 0.5581 - val_accuracy: 0.7222\n",
      "Epoch 386/1000\n",
      "68/68 [==============================] - 0s 388us/step - loss: 0.6044 - accuracy: 0.6765 - val_loss: 0.5924 - val_accuracy: 0.6667\n",
      "Epoch 387/1000\n",
      "68/68 [==============================] - 0s 158us/step - loss: 0.6099 - accuracy: 0.6765 - val_loss: 0.5704 - val_accuracy: 0.7222\n",
      "Epoch 388/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5988 - accuracy: 0.6765 - val_loss: 0.5689 - val_accuracy: 0.6667\n",
      "Epoch 389/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5941 - accuracy: 0.6765 - val_loss: 0.5399 - val_accuracy: 0.7222\n",
      "Epoch 390/1000\n",
      "68/68 [==============================] - 0s 163us/step - loss: 0.5876 - accuracy: 0.6912 - val_loss: 0.5175 - val_accuracy: 0.7778\n",
      "Epoch 391/1000\n",
      "68/68 [==============================] - 0s 154us/step - loss: 0.5902 - accuracy: 0.7059 - val_loss: 0.5239 - val_accuracy: 0.7778\n",
      "Epoch 392/1000\n",
      "68/68 [==============================] - 0s 154us/step - loss: 0.5966 - accuracy: 0.6912 - val_loss: 0.5264 - val_accuracy: 0.7778\n",
      "Epoch 393/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5996 - accuracy: 0.6912 - val_loss: 0.5280 - val_accuracy: 0.7778\n",
      "Epoch 394/1000\n",
      "68/68 [==============================] - 0s 171us/step - loss: 0.5958 - accuracy: 0.6912 - val_loss: 0.5268 - val_accuracy: 0.7778\n",
      "Epoch 395/1000\n",
      "68/68 [==============================] - 0s 114us/step - loss: 0.5939 - accuracy: 0.7059 - val_loss: 0.5459 - val_accuracy: 0.7778\n",
      "Epoch 396/1000\n",
      "68/68 [==============================] - 0s 182us/step - loss: 0.5928 - accuracy: 0.6765 - val_loss: 0.5641 - val_accuracy: 0.7222\n",
      "Epoch 397/1000\n",
      "68/68 [==============================] - 0s 159us/step - loss: 0.5949 - accuracy: 0.6912 - val_loss: 0.5300 - val_accuracy: 0.7222\n",
      "Epoch 398/1000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.6555 - accuracy: 0.71 - 0s 135us/step - loss: 0.5865 - accuracy: 0.7206 - val_loss: 0.4954 - val_accuracy: 0.8333\n",
      "Epoch 399/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5878 - accuracy: 0.6912 - val_loss: 0.4884 - val_accuracy: 0.8333\n",
      "Epoch 400/1000\n",
      "68/68 [==============================] - 0s 89us/step - loss: 0.5903 - accuracy: 0.6765 - val_loss: 0.4830 - val_accuracy: 0.8333\n",
      "Epoch 401/1000\n",
      "68/68 [==============================] - 0s 262us/step - loss: 0.5934 - accuracy: 0.6912 - val_loss: 0.4827 - val_accuracy: 0.8333\n",
      "Epoch 402/1000\n",
      "68/68 [==============================] - 0s 167us/step - loss: 0.5952 - accuracy: 0.6765 - val_loss: 0.4857 - val_accuracy: 0.8333\n",
      "Epoch 403/1000\n",
      "68/68 [==============================] - 0s 163us/step - loss: 0.5936 - accuracy: 0.6912 - val_loss: 0.5095 - val_accuracy: 0.8333\n",
      "Epoch 404/1000\n",
      "68/68 [==============================] - 0s 114us/step - loss: 0.5896 - accuracy: 0.6912 - val_loss: 0.5461 - val_accuracy: 0.7222\n",
      "Epoch 405/1000\n",
      "68/68 [==============================] - 0s 178us/step - loss: 0.5884 - accuracy: 0.7206 - val_loss: 0.5460 - val_accuracy: 0.7222\n",
      "Epoch 406/1000\n",
      "68/68 [==============================] - 0s 269us/step - loss: 0.5876 - accuracy: 0.7206 - val_loss: 0.5336 - val_accuracy: 0.7222\n",
      "Epoch 407/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5848 - accuracy: 0.7059 - val_loss: 0.5215 - val_accuracy: 0.7222\n",
      "Epoch 408/1000\n",
      "68/68 [==============================] - 0s 221us/step - loss: 0.5872 - accuracy: 0.7059 - val_loss: 0.5160 - val_accuracy: 0.7778\n",
      "Epoch 409/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5913 - accuracy: 0.7059 - val_loss: 0.5304 - val_accuracy: 0.7778\n",
      "Epoch 410/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5859 - accuracy: 0.7059 - val_loss: 0.5217 - val_accuracy: 0.7778\n",
      "Epoch 411/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.5847 - accuracy: 0.6912 - val_loss: 0.5127 - val_accuracy: 0.7778\n",
      "Epoch 412/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5835 - accuracy: 0.6912 - val_loss: 0.5151 - val_accuracy: 0.7778\n",
      "Epoch 413/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.5844 - accuracy: 0.7059 - val_loss: 0.5358 - val_accuracy: 0.7222\n",
      "Epoch 414/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5879 - accuracy: 0.7206 - val_loss: 0.5470 - val_accuracy: 0.7222\n",
      "Epoch 415/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5884 - accuracy: 0.7059 - val_loss: 0.5480 - val_accuracy: 0.7222\n",
      "Epoch 416/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5892 - accuracy: 0.7059 - val_loss: 0.5516 - val_accuracy: 0.7222\n",
      "Epoch 417/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5926 - accuracy: 0.7206 - val_loss: 0.5531 - val_accuracy: 0.6667\n",
      "Epoch 418/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5958 - accuracy: 0.7206 - val_loss: 0.5513 - val_accuracy: 0.7222\n",
      "Epoch 419/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5968 - accuracy: 0.7206 - val_loss: 0.5450 - val_accuracy: 0.7222\n",
      "Epoch 420/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5947 - accuracy: 0.7206 - val_loss: 0.5361 - val_accuracy: 0.7222\n",
      "Epoch 421/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5918 - accuracy: 0.7206 - val_loss: 0.5463 - val_accuracy: 0.7222\n",
      "Epoch 422/1000\n",
      "68/68 [==============================] - 0s 91us/step - loss: 0.5921 - accuracy: 0.7206 - val_loss: 0.5503 - val_accuracy: 0.6667\n",
      "Epoch 423/1000\n",
      "68/68 [==============================] - 0s 249us/step - loss: 0.5920 - accuracy: 0.7059 - val_loss: 0.5170 - val_accuracy: 0.7778\n",
      "Epoch 424/1000\n",
      "68/68 [==============================] - 0s 167us/step - loss: 0.5833 - accuracy: 0.7059 - val_loss: 0.5064 - val_accuracy: 0.7778\n",
      "Epoch 425/1000\n",
      "68/68 [==============================] - 0s 185us/step - loss: 0.5831 - accuracy: 0.7206 - val_loss: 0.5041 - val_accuracy: 0.7778\n",
      "Epoch 426/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5854 - accuracy: 0.7059 - val_loss: 0.5152 - val_accuracy: 0.7778\n",
      "Epoch 427/1000\n",
      "68/68 [==============================] - 0s 156us/step - loss: 0.5822 - accuracy: 0.7059 - val_loss: 0.5169 - val_accuracy: 0.7778\n",
      "Epoch 428/1000\n",
      "68/68 [==============================] - 0s 166us/step - loss: 0.5813 - accuracy: 0.7059 - val_loss: 0.5175 - val_accuracy: 0.7778\n",
      "Epoch 429/1000\n",
      "68/68 [==============================] - 0s 143us/step - loss: 0.5807 - accuracy: 0.7059 - val_loss: 0.5305 - val_accuracy: 0.7222\n",
      "Epoch 430/1000\n",
      "68/68 [==============================] - 0s 135us/step - loss: 0.5835 - accuracy: 0.7059 - val_loss: 0.5411 - val_accuracy: 0.7222\n",
      "Epoch 431/1000\n",
      "68/68 [==============================] - 0s 168us/step - loss: 0.5838 - accuracy: 0.7059 - val_loss: 0.5281 - val_accuracy: 0.7222\n",
      "Epoch 432/1000\n",
      "68/68 [==============================] - 0s 76us/step - loss: 0.5780 - accuracy: 0.7059 - val_loss: 0.5046 - val_accuracy: 0.7778\n",
      "Epoch 433/1000\n",
      "68/68 [==============================] - 0s 298us/step - loss: 0.5778 - accuracy: 0.7206 - val_loss: 0.4872 - val_accuracy: 0.8333\n",
      "Epoch 434/1000\n",
      "68/68 [==============================] - 0s 167us/step - loss: 0.5866 - accuracy: 0.6912 - val_loss: 0.4988 - val_accuracy: 0.7778\n",
      "Epoch 435/1000\n",
      "68/68 [==============================] - 0s 177us/step - loss: 0.5760 - accuracy: 0.6912 - val_loss: 0.5584 - val_accuracy: 0.7222\n",
      "Epoch 436/1000\n",
      "68/68 [==============================] - 0s 145us/step - loss: 0.5905 - accuracy: 0.6765 - val_loss: 0.6130 - val_accuracy: 0.6111\n",
      "Epoch 437/1000\n",
      "68/68 [==============================] - 0s 102us/step - loss: 0.6098 - accuracy: 0.6765 - val_loss: 0.5771 - val_accuracy: 0.6667\n",
      "Epoch 438/1000\n",
      "68/68 [==============================] - 0s 256us/step - loss: 0.5910 - accuracy: 0.6765 - val_loss: 0.5171 - val_accuracy: 0.7222\n",
      "Epoch 439/1000\n",
      "68/68 [==============================] - 0s 103us/step - loss: 0.5841 - accuracy: 0.6912 - val_loss: 0.4719 - val_accuracy: 0.8333\n",
      "Epoch 440/1000\n",
      "68/68 [==============================] - 0s 222us/step - loss: 0.6026 - accuracy: 0.6765 - val_loss: 0.4587 - val_accuracy: 0.8333\n",
      "Epoch 441/1000\n",
      "68/68 [==============================] - 0s 101us/step - loss: 0.6265 - accuracy: 0.6618 - val_loss: 0.4542 - val_accuracy: 0.8333\n",
      "Epoch 442/1000\n",
      "68/68 [==============================] - 0s 273us/step - loss: 0.6256 - accuracy: 0.6618 - val_loss: 0.4649 - val_accuracy: 0.8333\n",
      "Epoch 443/1000\n",
      "68/68 [==============================] - 0s 152us/step - loss: 0.5882 - accuracy: 0.6912 - val_loss: 0.5057 - val_accuracy: 0.7778\n",
      "Epoch 444/1000\n",
      "68/68 [==============================] - 0s 160us/step - loss: 0.5855 - accuracy: 0.7059 - val_loss: 0.5453 - val_accuracy: 0.7222\n",
      "Epoch 445/1000\n",
      "68/68 [==============================] - 0s 171us/step - loss: 0.5895 - accuracy: 0.7059 - val_loss: 0.5290 - val_accuracy: 0.7222\n",
      "Epoch 446/1000\n",
      "68/68 [==============================] - 0s 168us/step - loss: 0.5842 - accuracy: 0.7059 - val_loss: 0.5010 - val_accuracy: 0.7778\n",
      "Epoch 447/1000\n",
      "68/68 [==============================] - 0s 125us/step - loss: 0.5961 - accuracy: 0.7206 - val_loss: 0.4702 - val_accuracy: 0.7778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 448/1000\n",
      "68/68 [==============================] - 0s 166us/step - loss: 0.5916 - accuracy: 0.6912 - val_loss: 0.4828 - val_accuracy: 0.7778\n",
      "Epoch 449/1000\n",
      "68/68 [==============================] - 0s 170us/step - loss: 0.5802 - accuracy: 0.7059 - val_loss: 0.4966 - val_accuracy: 0.7778\n",
      "Epoch 450/1000\n",
      "68/68 [==============================] - 0s 85us/step - loss: 0.5774 - accuracy: 0.7059 - val_loss: 0.5016 - val_accuracy: 0.7778\n",
      "Epoch 451/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.5830 - accuracy: 0.6912 - val_loss: 0.5014 - val_accuracy: 0.7778\n",
      "Epoch 452/1000\n",
      "68/68 [==============================] - 0s 256us/step - loss: 0.5806 - accuracy: 0.6912 - val_loss: 0.5142 - val_accuracy: 0.7778\n",
      "Epoch 453/1000\n",
      "68/68 [==============================] - 0s 144us/step - loss: 0.5809 - accuracy: 0.7059 - val_loss: 0.5322 - val_accuracy: 0.7222\n",
      "Epoch 454/1000\n",
      "68/68 [==============================] - 0s 166us/step - loss: 0.5829 - accuracy: 0.7059 - val_loss: 0.5342 - val_accuracy: 0.7222\n",
      "Epoch 455/1000\n",
      "68/68 [==============================] - 0s 146us/step - loss: 0.5789 - accuracy: 0.7206 - val_loss: 0.5244 - val_accuracy: 0.7222\n",
      "Epoch 456/1000\n",
      "68/68 [==============================] - 0s 156us/step - loss: 0.5759 - accuracy: 0.7206 - val_loss: 0.5177 - val_accuracy: 0.7222\n",
      "Epoch 457/1000\n",
      "68/68 [==============================] - 0s 127us/step - loss: 0.5754 - accuracy: 0.7206 - val_loss: 0.5073 - val_accuracy: 0.7778\n",
      "Epoch 458/1000\n",
      "68/68 [==============================] - 0s 153us/step - loss: 0.5743 - accuracy: 0.7059 - val_loss: 0.4944 - val_accuracy: 0.7778\n",
      "Epoch 459/1000\n",
      "68/68 [==============================] - 0s 113us/step - loss: 0.5793 - accuracy: 0.6912 - val_loss: 0.4929 - val_accuracy: 0.7778\n",
      "Epoch 460/1000\n",
      "68/68 [==============================] - 0s 227us/step - loss: 0.5770 - accuracy: 0.6912 - val_loss: 0.5022 - val_accuracy: 0.7778\n",
      "Epoch 461/1000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.4791 - accuracy: 0.78 - 0s 140us/step - loss: 0.5739 - accuracy: 0.7206 - val_loss: 0.5162 - val_accuracy: 0.7778\n",
      "Epoch 462/1000\n",
      "68/68 [==============================] - 0s 169us/step - loss: 0.5736 - accuracy: 0.7206 - val_loss: 0.5191 - val_accuracy: 0.7778\n",
      "Epoch 463/1000\n",
      "68/68 [==============================] - 0s 154us/step - loss: 0.5746 - accuracy: 0.7206 - val_loss: 0.5195 - val_accuracy: 0.7778\n",
      "Epoch 464/1000\n",
      "68/68 [==============================] - 0s 197us/step - loss: 0.5757 - accuracy: 0.7206 - val_loss: 0.5206 - val_accuracy: 0.7778\n",
      "Epoch 465/1000\n",
      "68/68 [==============================] - 0s 168us/step - loss: 0.5761 - accuracy: 0.7206 - val_loss: 0.5157 - val_accuracy: 0.7778\n",
      "Epoch 466/1000\n",
      "68/68 [==============================] - 0s 158us/step - loss: 0.5772 - accuracy: 0.7206 - val_loss: 0.5246 - val_accuracy: 0.7778\n",
      "Epoch 467/1000\n",
      "68/68 [==============================] - 0s 172us/step - loss: 0.5761 - accuracy: 0.7206 - val_loss: 0.5159 - val_accuracy: 0.7778\n",
      "Epoch 468/1000\n",
      "68/68 [==============================] - 0s 63us/step - loss: 0.5762 - accuracy: 0.7206 - val_loss: 0.5025 - val_accuracy: 0.7778\n",
      "Epoch 469/1000\n",
      "68/68 [==============================] - 0s 231us/step - loss: 0.5791 - accuracy: 0.7059 - val_loss: 0.4798 - val_accuracy: 0.8333\n",
      "Epoch 470/1000\n",
      "68/68 [==============================] - 0s 165us/step - loss: 0.5867 - accuracy: 0.6912 - val_loss: 0.4834 - val_accuracy: 0.7778\n",
      "Epoch 471/1000\n",
      "68/68 [==============================] - 0s 182us/step - loss: 0.5816 - accuracy: 0.7059 - val_loss: 0.5159 - val_accuracy: 0.7778\n",
      "Epoch 472/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5764 - accuracy: 0.7059 - val_loss: 0.5484 - val_accuracy: 0.6667\n",
      "Epoch 473/1000\n",
      "68/68 [==============================] - 0s 189us/step - loss: 0.5810 - accuracy: 0.7206 - val_loss: 0.5615 - val_accuracy: 0.6667\n",
      "Epoch 474/1000\n",
      "68/68 [==============================] - 0s 195us/step - loss: 0.5817 - accuracy: 0.7206 - val_loss: 0.5208 - val_accuracy: 0.7778\n",
      "Epoch 475/1000\n",
      "68/68 [==============================] - 0s 200us/step - loss: 0.5999 - accuracy: 0.6618 - val_loss: 0.4973 - val_accuracy: 0.7778\n",
      "Epoch 476/1000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.5880 - accuracy: 0.71 - 0s 164us/step - loss: 0.5884 - accuracy: 0.7059 - val_loss: 0.5302 - val_accuracy: 0.7778\n",
      "Epoch 477/1000\n",
      "68/68 [==============================] - 0s 164us/step - loss: 0.5775 - accuracy: 0.7206 - val_loss: 0.5535 - val_accuracy: 0.7222\n",
      "Epoch 478/1000\n",
      "68/68 [==============================] - 0s 110us/step - loss: 0.5797 - accuracy: 0.7059 - val_loss: 0.5804 - val_accuracy: 0.6667\n",
      "Epoch 479/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5951 - accuracy: 0.6765 - val_loss: 0.6084 - val_accuracy: 0.6667\n",
      "Epoch 480/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6053 - accuracy: 0.6912 - val_loss: 0.5959 - val_accuracy: 0.6667\n",
      "Epoch 481/1000\n",
      "68/68 [==============================] - 0s 138us/step - loss: 0.5978 - accuracy: 0.6618 - val_loss: 0.5671 - val_accuracy: 0.7222\n",
      "Epoch 482/1000\n",
      "68/68 [==============================] - 0s 230us/step - loss: 0.5897 - accuracy: 0.6912 - val_loss: 0.5264 - val_accuracy: 0.7778\n",
      "Epoch 483/1000\n",
      "68/68 [==============================] - 0s 222us/step - loss: 0.5825 - accuracy: 0.6765 - val_loss: 0.5017 - val_accuracy: 0.8333\n",
      "Epoch 484/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5989 - accuracy: 0.7059 - val_loss: 0.4898 - val_accuracy: 0.8333\n",
      "Epoch 485/1000\n",
      "68/68 [==============================] - 0s 150us/step - loss: 0.6046 - accuracy: 0.6765 - val_loss: 0.4801 - val_accuracy: 0.8333\n",
      "Epoch 486/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6003 - accuracy: 0.6471 - val_loss: 0.4731 - val_accuracy: 0.8333\n",
      "Epoch 487/1000\n",
      "68/68 [==============================] - 0s 93us/step - loss: 0.5890 - accuracy: 0.7059 - val_loss: 0.4782 - val_accuracy: 0.8333\n",
      "Epoch 488/1000\n",
      "68/68 [==============================] - 0s 276us/step - loss: 0.5740 - accuracy: 0.6912 - val_loss: 0.4952 - val_accuracy: 0.7778\n",
      "Epoch 489/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5717 - accuracy: 0.7206 - val_loss: 0.5323 - val_accuracy: 0.7222\n",
      "Epoch 490/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.5789 - accuracy: 0.7059 - val_loss: 0.5563 - val_accuracy: 0.7222\n",
      "Epoch 491/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5856 - accuracy: 0.6912 - val_loss: 0.5503 - val_accuracy: 0.6667\n",
      "Epoch 492/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5818 - accuracy: 0.7059 - val_loss: 0.5087 - val_accuracy: 0.7778\n",
      "Epoch 493/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5709 - accuracy: 0.7206 - val_loss: 0.4774 - val_accuracy: 0.7778\n",
      "Epoch 494/1000\n",
      "68/68 [==============================] - 0s 165us/step - loss: 0.5776 - accuracy: 0.7059 - val_loss: 0.4744 - val_accuracy: 0.8333\n",
      "Epoch 495/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5741 - accuracy: 0.6912 - val_loss: 0.4933 - val_accuracy: 0.7778\n",
      "Epoch 496/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5723 - accuracy: 0.7206 - val_loss: 0.5144 - val_accuracy: 0.7222\n",
      "Epoch 497/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5769 - accuracy: 0.7206 - val_loss: 0.5484 - val_accuracy: 0.7222\n",
      "Epoch 498/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5885 - accuracy: 0.7059 - val_loss: 0.5642 - val_accuracy: 0.6667\n",
      "Epoch 499/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5851 - accuracy: 0.7206 - val_loss: 0.5381 - val_accuracy: 0.7222\n",
      "Epoch 500/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5736 - accuracy: 0.7059 - val_loss: 0.5251 - val_accuracy: 0.7778\n",
      "Epoch 501/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.5698 - accuracy: 0.7059 - val_loss: 0.5098 - val_accuracy: 0.7778\n",
      "Epoch 502/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5674 - accuracy: 0.7206 - val_loss: 0.4976 - val_accuracy: 0.7778\n",
      "Epoch 503/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.5699 - accuracy: 0.7206 - val_loss: 0.4881 - val_accuracy: 0.7778\n",
      "Epoch 504/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5745 - accuracy: 0.7059 - val_loss: 0.4961 - val_accuracy: 0.7778\n",
      "Epoch 505/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5700 - accuracy: 0.7206 - val_loss: 0.5185 - val_accuracy: 0.7778\n",
      "Epoch 506/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5686 - accuracy: 0.7206 - val_loss: 0.5325 - val_accuracy: 0.7222\n",
      "Epoch 507/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.5720 - accuracy: 0.7059 - val_loss: 0.5670 - val_accuracy: 0.7222\n",
      "Epoch 508/1000\n",
      "68/68 [==============================] - 0s 89us/step - loss: 0.5862 - accuracy: 0.6765 - val_loss: 0.5677 - val_accuracy: 0.6667\n",
      "Epoch 509/1000\n",
      "68/68 [==============================] - 0s 230us/step - loss: 0.5802 - accuracy: 0.6912 - val_loss: 0.5163 - val_accuracy: 0.7222\n",
      "Epoch 510/1000\n",
      "68/68 [==============================] - 0s 220us/step - loss: 0.5728 - accuracy: 0.7059 - val_loss: 0.4821 - val_accuracy: 0.8333\n",
      "Epoch 511/1000\n",
      "68/68 [==============================] - 0s 151us/step - loss: 0.5757 - accuracy: 0.7059 - val_loss: 0.4829 - val_accuracy: 0.8333\n",
      "Epoch 512/1000\n",
      "68/68 [==============================] - 0s 179us/step - loss: 0.5719 - accuracy: 0.7059 - val_loss: 0.5058 - val_accuracy: 0.7778\n",
      "Epoch 513/1000\n",
      "68/68 [==============================] - 0s 171us/step - loss: 0.5662 - accuracy: 0.7206 - val_loss: 0.5252 - val_accuracy: 0.7778\n",
      "Epoch 514/1000\n",
      "68/68 [==============================] - 0s 96us/step - loss: 0.5691 - accuracy: 0.7059 - val_loss: 0.5260 - val_accuracy: 0.7778\n",
      "Epoch 515/1000\n",
      "68/68 [==============================] - 0s 235us/step - loss: 0.5683 - accuracy: 0.7059 - val_loss: 0.5047 - val_accuracy: 0.7778\n",
      "Epoch 516/1000\n",
      "68/68 [==============================] - 0s 174us/step - loss: 0.5685 - accuracy: 0.7059 - val_loss: 0.4917 - val_accuracy: 0.7778\n",
      "Epoch 517/1000\n",
      "68/68 [==============================] - 0s 166us/step - loss: 0.5687 - accuracy: 0.7206 - val_loss: 0.5018 - val_accuracy: 0.7778\n",
      "Epoch 518/1000\n",
      "68/68 [==============================] - 0s 169us/step - loss: 0.5665 - accuracy: 0.7206 - val_loss: 0.5141 - val_accuracy: 0.7778\n",
      "Epoch 519/1000\n",
      "68/68 [==============================] - 0s 154us/step - loss: 0.5677 - accuracy: 0.7059 - val_loss: 0.5226 - val_accuracy: 0.7778\n",
      "Epoch 520/1000\n",
      "68/68 [==============================] - 0s 142us/step - loss: 0.5686 - accuracy: 0.7059 - val_loss: 0.5277 - val_accuracy: 0.7222\n",
      "Epoch 521/1000\n",
      "68/68 [==============================] - 0s 158us/step - loss: 0.5741 - accuracy: 0.7059 - val_loss: 0.5575 - val_accuracy: 0.7222\n",
      "Epoch 522/1000\n",
      "68/68 [==============================] - 0s 152us/step - loss: 0.5816 - accuracy: 0.6912 - val_loss: 0.5592 - val_accuracy: 0.7222\n",
      "Epoch 523/1000\n",
      "68/68 [==============================] - 0s 161us/step - loss: 0.5830 - accuracy: 0.6765 - val_loss: 0.5658 - val_accuracy: 0.7222\n",
      "Epoch 524/1000\n",
      "68/68 [==============================] - 0s 174us/step - loss: 0.5815 - accuracy: 0.6765 - val_loss: 0.5457 - val_accuracy: 0.7222\n",
      "Epoch 525/1000\n",
      "68/68 [==============================] - 0s 161us/step - loss: 0.5700 - accuracy: 0.7059 - val_loss: 0.5029 - val_accuracy: 0.7778\n",
      "Epoch 526/1000\n",
      "68/68 [==============================] - 0s 178us/step - loss: 0.5689 - accuracy: 0.7206 - val_loss: 0.4833 - val_accuracy: 0.7778\n",
      "Epoch 527/1000\n",
      "68/68 [==============================] - 0s 169us/step - loss: 0.5697 - accuracy: 0.7059 - val_loss: 0.4997 - val_accuracy: 0.7778\n",
      "Epoch 528/1000\n",
      "68/68 [==============================] - 0s 146us/step - loss: 0.5700 - accuracy: 0.7206 - val_loss: 0.5125 - val_accuracy: 0.7778\n",
      "Epoch 529/1000\n",
      "68/68 [==============================] - 0s 174us/step - loss: 0.5686 - accuracy: 0.7206 - val_loss: 0.5025 - val_accuracy: 0.7778\n",
      "Epoch 530/1000\n",
      "68/68 [==============================] - 0s 164us/step - loss: 0.5746 - accuracy: 0.7206 - val_loss: 0.4975 - val_accuracy: 0.7778\n",
      "Epoch 531/1000\n",
      "68/68 [==============================] - 0s 139us/step - loss: 0.5742 - accuracy: 0.7059 - val_loss: 0.4954 - val_accuracy: 0.7778\n",
      "Epoch 532/1000\n",
      "68/68 [==============================] - 0s 151us/step - loss: 0.5704 - accuracy: 0.7353 - val_loss: 0.4904 - val_accuracy: 0.7778\n",
      "Epoch 533/1000\n",
      "68/68 [==============================] - 0s 155us/step - loss: 0.5706 - accuracy: 0.7353 - val_loss: 0.4980 - val_accuracy: 0.7778\n",
      "Epoch 534/1000\n",
      "68/68 [==============================] - 0s 165us/step - loss: 0.5715 - accuracy: 0.6912 - val_loss: 0.5016 - val_accuracy: 0.7778\n",
      "Epoch 535/1000\n",
      "68/68 [==============================] - 0s 234us/step - loss: 0.5681 - accuracy: 0.6912 - val_loss: 0.4700 - val_accuracy: 0.8333\n",
      "Epoch 536/1000\n",
      "68/68 [==============================] - 0s 288us/step - loss: 0.5887 - accuracy: 0.6912 - val_loss: 0.4583 - val_accuracy: 0.7222\n",
      "Epoch 537/1000\n",
      "68/68 [==============================] - 0s 336us/step - loss: 0.6044 - accuracy: 0.6618 - val_loss: 0.4570 - val_accuracy: 0.8333\n",
      "Epoch 538/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5901 - accuracy: 0.6765 - val_loss: 0.4698 - val_accuracy: 0.8333\n",
      "Epoch 539/1000\n",
      "68/68 [==============================] - 0s 274us/step - loss: 0.5711 - accuracy: 0.6765 - val_loss: 0.5139 - val_accuracy: 0.7778\n",
      "Epoch 540/1000\n",
      "68/68 [==============================] - 0s 375us/step - loss: 0.5696 - accuracy: 0.7059 - val_loss: 0.5616 - val_accuracy: 0.6667\n",
      "Epoch 541/1000\n",
      "68/68 [==============================] - 0s 174us/step - loss: 0.5830 - accuracy: 0.7206 - val_loss: 0.5664 - val_accuracy: 0.6667\n",
      "Epoch 542/1000\n",
      "68/68 [==============================] - 0s 262us/step - loss: 0.5737 - accuracy: 0.7206 - val_loss: 0.5213 - val_accuracy: 0.7222\n",
      "Epoch 543/1000\n",
      "68/68 [==============================] - 0s 256us/step - loss: 0.5659 - accuracy: 0.7059 - val_loss: 0.4775 - val_accuracy: 0.7778\n",
      "Epoch 544/1000\n",
      "68/68 [==============================] - 0s 235us/step - loss: 0.5782 - accuracy: 0.6912 - val_loss: 0.4639 - val_accuracy: 0.8333\n",
      "Epoch 545/1000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.6275 - accuracy: 0.62 - 0s 230us/step - loss: 0.5814 - accuracy: 0.6912 - val_loss: 0.4749 - val_accuracy: 0.7778\n",
      "Epoch 546/1000\n",
      "68/68 [==============================] - 0s 173us/step - loss: 0.5692 - accuracy: 0.7059 - val_loss: 0.4957 - val_accuracy: 0.7778\n",
      "Epoch 547/1000\n",
      "68/68 [==============================] - 0s 330us/step - loss: 0.5681 - accuracy: 0.7206 - val_loss: 0.5299 - val_accuracy: 0.7222\n",
      "Epoch 548/1000\n",
      "68/68 [==============================] - 0s 153us/step - loss: 0.5776 - accuracy: 0.6912 - val_loss: 0.5827 - val_accuracy: 0.6667\n",
      "Epoch 549/1000\n",
      "68/68 [==============================] - 0s 374us/step - loss: 0.6010 - accuracy: 0.6765 - val_loss: 0.5512 - val_accuracy: 0.7222\n",
      "Epoch 550/1000\n",
      "68/68 [==============================] - 0s 281us/step - loss: 0.5792 - accuracy: 0.6912 - val_loss: 0.4970 - val_accuracy: 0.8333\n",
      "Epoch 551/1000\n",
      "68/68 [==============================] - 0s 230us/step - loss: 0.5670 - accuracy: 0.7059 - val_loss: 0.4736 - val_accuracy: 0.8333\n",
      "Epoch 552/1000\n",
      "68/68 [==============================] - 0s 163us/step - loss: 0.5731 - accuracy: 0.6912 - val_loss: 0.4635 - val_accuracy: 0.8333\n",
      "Epoch 553/1000\n",
      "68/68 [==============================] - 0s 230us/step - loss: 0.5784 - accuracy: 0.6912 - val_loss: 0.4696 - val_accuracy: 0.8333\n",
      "Epoch 554/1000\n",
      "68/68 [==============================] - 0s 159us/step - loss: 0.5704 - accuracy: 0.6912 - val_loss: 0.4929 - val_accuracy: 0.7778\n",
      "Epoch 555/1000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.5344 - accuracy: 0.65 - 0s 181us/step - loss: 0.5636 - accuracy: 0.7059 - val_loss: 0.5335 - val_accuracy: 0.7222\n",
      "Epoch 556/1000\n",
      "68/68 [==============================] - 0s 190us/step - loss: 0.5666 - accuracy: 0.7059 - val_loss: 0.5602 - val_accuracy: 0.6667\n",
      "Epoch 557/1000\n",
      "68/68 [==============================] - 0s 261us/step - loss: 0.5753 - accuracy: 0.7059 - val_loss: 0.5555 - val_accuracy: 0.6667\n",
      "Epoch 558/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 140us/step - loss: 0.5707 - accuracy: 0.7059 - val_loss: 0.5270 - val_accuracy: 0.7778\n",
      "Epoch 559/1000\n",
      "68/68 [==============================] - 0s 253us/step - loss: 0.5658 - accuracy: 0.7206 - val_loss: 0.5237 - val_accuracy: 0.7778\n",
      "Epoch 560/1000\n",
      "68/68 [==============================] - 0s 295us/step - loss: 0.5682 - accuracy: 0.6912 - val_loss: 0.5277 - val_accuracy: 0.7222\n",
      "Epoch 561/1000\n",
      "68/68 [==============================] - 0s 330us/step - loss: 0.5714 - accuracy: 0.6912 - val_loss: 0.5210 - val_accuracy: 0.7778\n",
      "Epoch 562/1000\n",
      "68/68 [==============================] - 0s 274us/step - loss: 0.5720 - accuracy: 0.7059 - val_loss: 0.5046 - val_accuracy: 0.7778\n",
      "Epoch 563/1000\n",
      "68/68 [==============================] - 0s 313us/step - loss: 0.5912 - accuracy: 0.7059 - val_loss: 0.4911 - val_accuracy: 0.7778\n",
      "Epoch 564/1000\n",
      "68/68 [==============================] - 0s 270us/step - loss: 0.5896 - accuracy: 0.7059 - val_loss: 0.5001 - val_accuracy: 0.7778\n",
      "Epoch 565/1000\n",
      "68/68 [==============================] - 0s 297us/step - loss: 0.5818 - accuracy: 0.6912 - val_loss: 0.5413 - val_accuracy: 0.7222\n",
      "Epoch 566/1000\n",
      "68/68 [==============================] - 0s 288us/step - loss: 0.5705 - accuracy: 0.7353 - val_loss: 0.5625 - val_accuracy: 0.6667\n",
      "Epoch 567/1000\n",
      "68/68 [==============================] - 0s 292us/step - loss: 0.5743 - accuracy: 0.7059 - val_loss: 0.5491 - val_accuracy: 0.7222\n",
      "Epoch 568/1000\n",
      "68/68 [==============================] - 0s 411us/step - loss: 0.5666 - accuracy: 0.7059 - val_loss: 0.5036 - val_accuracy: 0.7778\n",
      "Epoch 569/1000\n",
      "68/68 [==============================] - 0s 192us/step - loss: 0.5587 - accuracy: 0.7206 - val_loss: 0.4768 - val_accuracy: 0.8333\n",
      "Epoch 570/1000\n",
      "68/68 [==============================] - 0s 241us/step - loss: 0.5706 - accuracy: 0.7059 - val_loss: 0.4678 - val_accuracy: 0.8333\n",
      "Epoch 571/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5793 - accuracy: 0.6912 - val_loss: 0.4674 - val_accuracy: 0.8333\n",
      "Epoch 572/1000\n",
      "68/68 [==============================] - 0s 175us/step - loss: 0.5737 - accuracy: 0.7059 - val_loss: 0.4761 - val_accuracy: 0.8333\n",
      "Epoch 573/1000\n",
      "68/68 [==============================] - 0s 265us/step - loss: 0.5678 - accuracy: 0.7059 - val_loss: 0.4947 - val_accuracy: 0.8333\n",
      "Epoch 574/1000\n",
      "68/68 [==============================] - 0s 264us/step - loss: 0.5651 - accuracy: 0.6912 - val_loss: 0.5200 - val_accuracy: 0.7222\n",
      "Epoch 575/1000\n",
      "68/68 [==============================] - 0s 227us/step - loss: 0.5717 - accuracy: 0.7059 - val_loss: 0.5638 - val_accuracy: 0.6667\n",
      "Epoch 576/1000\n",
      "68/68 [==============================] - 0s 221us/step - loss: 0.5843 - accuracy: 0.6765 - val_loss: 0.5414 - val_accuracy: 0.7222\n",
      "Epoch 577/1000\n",
      "68/68 [==============================] - 0s 230us/step - loss: 0.5673 - accuracy: 0.7206 - val_loss: 0.4787 - val_accuracy: 0.8333\n",
      "Epoch 578/1000\n",
      "68/68 [==============================] - 0s 257us/step - loss: 0.5698 - accuracy: 0.7059 - val_loss: 0.4598 - val_accuracy: 0.8333\n",
      "Epoch 579/1000\n",
      "68/68 [==============================] - 0s 213us/step - loss: 0.5810 - accuracy: 0.7059 - val_loss: 0.4705 - val_accuracy: 0.8333\n",
      "Epoch 580/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.5688 - accuracy: 0.6912 - val_loss: 0.4871 - val_accuracy: 0.8333\n",
      "Epoch 581/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.5652 - accuracy: 0.6912 - val_loss: 0.4927 - val_accuracy: 0.7778\n",
      "Epoch 582/1000\n",
      "68/68 [==============================] - 0s 91us/step - loss: 0.5636 - accuracy: 0.7059 - val_loss: 0.5034 - val_accuracy: 0.7778\n",
      "Epoch 583/1000\n",
      "68/68 [==============================] - 0s 211us/step - loss: 0.5680 - accuracy: 0.7206 - val_loss: 0.5101 - val_accuracy: 0.7778\n",
      "Epoch 584/1000\n",
      "68/68 [==============================] - 0s 199us/step - loss: 0.5621 - accuracy: 0.7206 - val_loss: 0.4865 - val_accuracy: 0.7778\n",
      "Epoch 585/1000\n",
      "68/68 [==============================] - 0s 221us/step - loss: 0.5669 - accuracy: 0.7059 - val_loss: 0.4731 - val_accuracy: 0.8333\n",
      "Epoch 586/1000\n",
      "68/68 [==============================] - 0s 178us/step - loss: 0.5670 - accuracy: 0.6765 - val_loss: 0.4796 - val_accuracy: 0.8333\n",
      "Epoch 587/1000\n",
      "68/68 [==============================] - 0s 146us/step - loss: 0.5621 - accuracy: 0.7059 - val_loss: 0.5033 - val_accuracy: 0.7778\n",
      "Epoch 588/1000\n",
      "68/68 [==============================] - 0s 181us/step - loss: 0.5636 - accuracy: 0.7059 - val_loss: 0.5294 - val_accuracy: 0.7222\n",
      "Epoch 589/1000\n",
      "68/68 [==============================] - 0s 169us/step - loss: 0.5626 - accuracy: 0.7059 - val_loss: 0.5096 - val_accuracy: 0.7778\n",
      "Epoch 590/1000\n",
      "68/68 [==============================] - 0s 166us/step - loss: 0.5611 - accuracy: 0.7353 - val_loss: 0.4863 - val_accuracy: 0.8333\n",
      "Epoch 591/1000\n",
      "68/68 [==============================] - 0s 159us/step - loss: 0.5620 - accuracy: 0.7059 - val_loss: 0.4901 - val_accuracy: 0.8333\n",
      "Epoch 592/1000\n",
      "68/68 [==============================] - 0s 155us/step - loss: 0.5609 - accuracy: 0.7059 - val_loss: 0.5189 - val_accuracy: 0.7222\n",
      "Epoch 593/1000\n",
      "68/68 [==============================] - 0s 80us/step - loss: 0.5574 - accuracy: 0.7206 - val_loss: 0.5694 - val_accuracy: 0.6667\n",
      "Epoch 594/1000\n",
      "68/68 [==============================] - 0s 259us/step - loss: 0.5834 - accuracy: 0.6912 - val_loss: 0.6362 - val_accuracy: 0.6111\n",
      "Epoch 595/1000\n",
      "68/68 [==============================] - 0s 189us/step - loss: 0.6243 - accuracy: 0.7206 - val_loss: 0.6358 - val_accuracy: 0.6111\n",
      "Epoch 596/1000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.7230 - accuracy: 0.65 - 0s 147us/step - loss: 0.6043 - accuracy: 0.6765 - val_loss: 0.5239 - val_accuracy: 0.7778\n",
      "Epoch 597/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.5597 - accuracy: 0.7206 - val_loss: 0.4749 - val_accuracy: 0.7778\n",
      "Epoch 598/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5975 - accuracy: 0.6618 - val_loss: 0.4594 - val_accuracy: 0.8333\n",
      "Epoch 599/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6280 - accuracy: 0.6765 - val_loss: 0.4669 - val_accuracy: 0.8333\n",
      "Epoch 600/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5969 - accuracy: 0.6618 - val_loss: 0.5449 - val_accuracy: 0.7222\n",
      "Epoch 601/1000\n",
      "68/68 [==============================] - 0s 206us/step - loss: 0.5722 - accuracy: 0.7206 - val_loss: 0.6203 - val_accuracy: 0.6111\n",
      "Epoch 602/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.6008 - accuracy: 0.6912 - val_loss: 0.5918 - val_accuracy: 0.6667\n",
      "Epoch 603/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5886 - accuracy: 0.6912 - val_loss: 0.5161 - val_accuracy: 0.7222\n",
      "Epoch 604/1000\n",
      "68/68 [==============================] - 0s 206us/step - loss: 0.5600 - accuracy: 0.7206 - val_loss: 0.4817 - val_accuracy: 0.8333\n",
      "Epoch 605/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5630 - accuracy: 0.6912 - val_loss: 0.4642 - val_accuracy: 0.8333\n",
      "Epoch 606/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5833 - accuracy: 0.7059 - val_loss: 0.4572 - val_accuracy: 0.8333\n",
      "Epoch 607/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.5819 - accuracy: 0.6912 - val_loss: 0.4668 - val_accuracy: 0.8333\n",
      "Epoch 608/1000\n",
      "68/68 [==============================] - 0s 149us/step - loss: 0.5637 - accuracy: 0.6912 - val_loss: 0.4933 - val_accuracy: 0.7778\n",
      "Epoch 609/1000\n",
      "68/68 [==============================] - 0s 170us/step - loss: 0.5609 - accuracy: 0.7059 - val_loss: 0.5483 - val_accuracy: 0.7222\n",
      "Epoch 610/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5818 - accuracy: 0.6912 - val_loss: 0.5681 - val_accuracy: 0.6667\n",
      "Epoch 611/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.5782 - accuracy: 0.6912 - val_loss: 0.5261 - val_accuracy: 0.7222\n",
      "Epoch 612/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5587 - accuracy: 0.7059 - val_loss: 0.4861 - val_accuracy: 0.7778\n",
      "Epoch 613/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.5536 - accuracy: 0.6912 - val_loss: 0.4580 - val_accuracy: 0.8333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 614/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5780 - accuracy: 0.7059 - val_loss: 0.4476 - val_accuracy: 0.8333\n",
      "Epoch 615/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5911 - accuracy: 0.6912 - val_loss: 0.4578 - val_accuracy: 0.8333\n",
      "Epoch 616/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5634 - accuracy: 0.7059 - val_loss: 0.5195 - val_accuracy: 0.7222\n",
      "Epoch 617/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5603 - accuracy: 0.7059 - val_loss: 0.6063 - val_accuracy: 0.6111\n",
      "Epoch 618/1000\n",
      "68/68 [==============================] - 0s 136us/step - loss: 0.6089 - accuracy: 0.6912 - val_loss: 0.6487 - val_accuracy: 0.6111\n",
      "Epoch 619/1000\n",
      "68/68 [==============================] - 0s 172us/step - loss: 0.6195 - accuracy: 0.6912 - val_loss: 0.5664 - val_accuracy: 0.6667\n",
      "Epoch 620/1000\n",
      "68/68 [==============================] - 0s 155us/step - loss: 0.5763 - accuracy: 0.6765 - val_loss: 0.4905 - val_accuracy: 0.7778\n",
      "Epoch 621/1000\n",
      "68/68 [==============================] - 0s 173us/step - loss: 0.5656 - accuracy: 0.7059 - val_loss: 0.4569 - val_accuracy: 0.7222\n",
      "Epoch 622/1000\n",
      "68/68 [==============================] - 0s 133us/step - loss: 0.5877 - accuracy: 0.7059 - val_loss: 0.4533 - val_accuracy: 0.7222\n",
      "Epoch 623/1000\n",
      "68/68 [==============================] - 0s 187us/step - loss: 0.5779 - accuracy: 0.6912 - val_loss: 0.4674 - val_accuracy: 0.8333\n",
      "Epoch 624/1000\n",
      "68/68 [==============================] - 0s 285us/step - loss: 0.5574 - accuracy: 0.6912 - val_loss: 0.5138 - val_accuracy: 0.7222\n",
      "Epoch 625/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.5656 - accuracy: 0.7059 - val_loss: 0.5629 - val_accuracy: 0.6667\n",
      "Epoch 626/1000\n",
      "68/68 [==============================] - 0s 161us/step - loss: 0.5818 - accuracy: 0.6765 - val_loss: 0.5718 - val_accuracy: 0.6667\n",
      "Epoch 627/1000\n",
      "68/68 [==============================] - 0s 170us/step - loss: 0.5871 - accuracy: 0.6765 - val_loss: 0.5835 - val_accuracy: 0.6111\n",
      "Epoch 628/1000\n",
      "68/68 [==============================] - 0s 149us/step - loss: 0.5877 - accuracy: 0.6765 - val_loss: 0.5515 - val_accuracy: 0.6667\n",
      "Epoch 629/1000\n",
      "68/68 [==============================] - 0s 146us/step - loss: 0.5735 - accuracy: 0.6912 - val_loss: 0.5133 - val_accuracy: 0.7222\n",
      "Epoch 630/1000\n",
      "68/68 [==============================] - 0s 154us/step - loss: 0.5606 - accuracy: 0.6765 - val_loss: 0.4838 - val_accuracy: 0.8333\n",
      "Epoch 631/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.5582 - accuracy: 0.6912 - val_loss: 0.4623 - val_accuracy: 0.8333\n",
      "Epoch 632/1000\n",
      "68/68 [==============================] - 0s 137us/step - loss: 0.5599 - accuracy: 0.7059 - val_loss: 0.4666 - val_accuracy: 0.8333\n",
      "Epoch 633/1000\n",
      "68/68 [==============================] - 0s 196us/step - loss: 0.5555 - accuracy: 0.6912 - val_loss: 0.4883 - val_accuracy: 0.7778\n",
      "Epoch 634/1000\n",
      "68/68 [==============================] - 0s 287us/step - loss: 0.5528 - accuracy: 0.7353 - val_loss: 0.5087 - val_accuracy: 0.7778\n",
      "Epoch 635/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5566 - accuracy: 0.7353 - val_loss: 0.5047 - val_accuracy: 0.7778\n",
      "Epoch 636/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5581 - accuracy: 0.7353 - val_loss: 0.4857 - val_accuracy: 0.7778\n",
      "Epoch 637/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5590 - accuracy: 0.7059 - val_loss: 0.4810 - val_accuracy: 0.7778\n",
      "Epoch 638/1000\n",
      "68/68 [==============================] - 0s 134us/step - loss: 0.5596 - accuracy: 0.6912 - val_loss: 0.4747 - val_accuracy: 0.8333\n",
      "Epoch 639/1000\n",
      "68/68 [==============================] - 0s 83us/step - loss: 0.5655 - accuracy: 0.6765 - val_loss: 0.4727 - val_accuracy: 0.8333\n",
      "Epoch 640/1000\n",
      "68/68 [==============================] - 0s 291us/step - loss: 0.5711 - accuracy: 0.6765 - val_loss: 0.4824 - val_accuracy: 0.8333\n",
      "Epoch 641/1000\n",
      "68/68 [==============================] - 0s 167us/step - loss: 0.5600 - accuracy: 0.6912 - val_loss: 0.5141 - val_accuracy: 0.7778\n",
      "Epoch 642/1000\n",
      "68/68 [==============================] - 0s 167us/step - loss: 0.5670 - accuracy: 0.7206 - val_loss: 0.5521 - val_accuracy: 0.6667\n",
      "Epoch 643/1000\n",
      "68/68 [==============================] - 0s 164us/step - loss: 0.5662 - accuracy: 0.7059 - val_loss: 0.5166 - val_accuracy: 0.7778\n",
      "Epoch 644/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5546 - accuracy: 0.7353 - val_loss: 0.4755 - val_accuracy: 0.7778\n",
      "Epoch 645/1000\n",
      "68/68 [==============================] - 0s 151us/step - loss: 0.5651 - accuracy: 0.7353 - val_loss: 0.4589 - val_accuracy: 0.8333\n",
      "Epoch 646/1000\n",
      "68/68 [==============================] - 0s 149us/step - loss: 0.5841 - accuracy: 0.6912 - val_loss: 0.4573 - val_accuracy: 0.8333\n",
      "Epoch 647/1000\n",
      "68/68 [==============================] - 0s 190us/step - loss: 0.5876 - accuracy: 0.7206 - val_loss: 0.4666 - val_accuracy: 0.8333\n",
      "Epoch 648/1000\n",
      "68/68 [==============================] - 0s 150us/step - loss: 0.5740 - accuracy: 0.6765 - val_loss: 0.4865 - val_accuracy: 0.8333\n",
      "Epoch 649/1000\n",
      "68/68 [==============================] - 0s 750us/step - loss: 0.5609 - accuracy: 0.6912 - val_loss: 0.5166 - val_accuracy: 0.7778\n",
      "Epoch 650/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.5589 - accuracy: 0.7353 - val_loss: 0.5310 - val_accuracy: 0.7222\n",
      "Epoch 651/1000\n",
      "68/68 [==============================] - 0s 138us/step - loss: 0.5630 - accuracy: 0.7206 - val_loss: 0.5086 - val_accuracy: 0.7222\n",
      "Epoch 652/1000\n",
      "68/68 [==============================] - 0s 0us/step - loss: 0.5560 - accuracy: 0.6912 - val_loss: 0.4747 - val_accuracy: 0.8333\n",
      "Epoch 653/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5627 - accuracy: 0.6765 - val_loss: 0.4634 - val_accuracy: 0.8333\n",
      "Epoch 654/1000\n",
      "68/68 [==============================] - 0s 53us/step - loss: 0.5769 - accuracy: 0.6912 - val_loss: 0.4585 - val_accuracy: 0.8333\n",
      "Epoch 655/1000\n",
      "68/68 [==============================] - 0s 319us/step - loss: 0.5716 - accuracy: 0.7059 - val_loss: 0.4810 - val_accuracy: 0.7778\n",
      "Epoch 656/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5579 - accuracy: 0.7353 - val_loss: 0.5275 - val_accuracy: 0.7222\n",
      "Epoch 657/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5610 - accuracy: 0.7206 - val_loss: 0.5550 - val_accuracy: 0.6667\n",
      "Epoch 658/1000\n",
      "68/68 [==============================] - 0s 221us/step - loss: 0.5740 - accuracy: 0.7206 - val_loss: 0.5750 - val_accuracy: 0.6667\n",
      "Epoch 659/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5761 - accuracy: 0.7059 - val_loss: 0.5489 - val_accuracy: 0.6667\n",
      "Epoch 660/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5591 - accuracy: 0.7059 - val_loss: 0.4934 - val_accuracy: 0.7778\n",
      "Epoch 661/1000\n",
      "68/68 [==============================] - 0s 75us/step - loss: 0.5516 - accuracy: 0.7206 - val_loss: 0.4496 - val_accuracy: 0.8333\n",
      "Epoch 662/1000\n",
      "68/68 [==============================] - 0s 243us/step - loss: 0.5896 - accuracy: 0.7206 - val_loss: 0.4474 - val_accuracy: 0.8333\n",
      "Epoch 663/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5846 - accuracy: 0.7206 - val_loss: 0.4637 - val_accuracy: 0.8333\n",
      "Epoch 664/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5688 - accuracy: 0.6912 - val_loss: 0.5240 - val_accuracy: 0.7222\n",
      "Epoch 665/1000\n",
      "68/68 [==============================] - 0s 137us/step - loss: 0.5567 - accuracy: 0.7059 - val_loss: 0.5823 - val_accuracy: 0.6667\n",
      "Epoch 666/1000\n",
      "68/68 [==============================] - 0s 197us/step - loss: 0.5850 - accuracy: 0.7059 - val_loss: 0.5956 - val_accuracy: 0.6667\n",
      "Epoch 667/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5810 - accuracy: 0.7206 - val_loss: 0.5505 - val_accuracy: 0.6667\n",
      "Epoch 668/1000\n",
      "68/68 [==============================] - 0s 167us/step - loss: 0.5615 - accuracy: 0.7353 - val_loss: 0.5094 - val_accuracy: 0.7778\n",
      "Epoch 669/1000\n",
      "68/68 [==============================] - 0s 167us/step - loss: 0.5479 - accuracy: 0.7206 - val_loss: 0.4702 - val_accuracy: 0.7778\n",
      "Epoch 670/1000\n",
      "68/68 [==============================] - 0s 171us/step - loss: 0.5739 - accuracy: 0.7059 - val_loss: 0.4507 - val_accuracy: 0.8333\n",
      "Epoch 671/1000\n",
      "68/68 [==============================] - 0s 198us/step - loss: 0.5932 - accuracy: 0.6618 - val_loss: 0.4535 - val_accuracy: 0.7778\n",
      "Epoch 672/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5766 - accuracy: 0.6912 - val_loss: 0.4713 - val_accuracy: 0.7778\n",
      "Epoch 673/1000\n",
      "68/68 [==============================] - 0s 177us/step - loss: 0.5579 - accuracy: 0.7206 - val_loss: 0.5002 - val_accuracy: 0.7222\n",
      "Epoch 674/1000\n",
      "68/68 [==============================] - 0s 187us/step - loss: 0.5533 - accuracy: 0.7059 - val_loss: 0.4997 - val_accuracy: 0.7778\n",
      "Epoch 675/1000\n",
      "68/68 [==============================] - 0s 165us/step - loss: 0.5528 - accuracy: 0.7206 - val_loss: 0.4976 - val_accuracy: 0.7778\n",
      "Epoch 676/1000\n",
      "68/68 [==============================] - 0s 163us/step - loss: 0.5530 - accuracy: 0.7206 - val_loss: 0.4716 - val_accuracy: 0.8333\n",
      "Epoch 677/1000\n",
      "68/68 [==============================] - 0s 156us/step - loss: 0.5513 - accuracy: 0.7059 - val_loss: 0.4491 - val_accuracy: 0.8333\n",
      "Epoch 678/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.5642 - accuracy: 0.6912 - val_loss: 0.4505 - val_accuracy: 0.8333\n",
      "Epoch 679/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5516 - accuracy: 0.6765 - val_loss: 0.4977 - val_accuracy: 0.7778\n",
      "Epoch 680/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.5575 - accuracy: 0.7206 - val_loss: 0.6030 - val_accuracy: 0.6111\n",
      "Epoch 681/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.6069 - accuracy: 0.7059 - val_loss: 0.6199 - val_accuracy: 0.6111\n",
      "Epoch 682/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6026 - accuracy: 0.7059 - val_loss: 0.5271 - val_accuracy: 0.7222\n",
      "Epoch 683/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5580 - accuracy: 0.6765 - val_loss: 0.4749 - val_accuracy: 0.8333\n",
      "Epoch 684/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5580 - accuracy: 0.6912 - val_loss: 0.4578 - val_accuracy: 0.8333\n",
      "Epoch 685/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5583 - accuracy: 0.7059 - val_loss: 0.4768 - val_accuracy: 0.8333\n",
      "Epoch 686/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5485 - accuracy: 0.6912 - val_loss: 0.4965 - val_accuracy: 0.7778\n",
      "Epoch 687/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5488 - accuracy: 0.7059 - val_loss: 0.4969 - val_accuracy: 0.7778\n",
      "Epoch 688/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5495 - accuracy: 0.7206 - val_loss: 0.5153 - val_accuracy: 0.7778\n",
      "Epoch 689/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5499 - accuracy: 0.7206 - val_loss: 0.5141 - val_accuracy: 0.7222\n",
      "Epoch 690/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5531 - accuracy: 0.7206 - val_loss: 0.5005 - val_accuracy: 0.7222\n",
      "Epoch 691/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5526 - accuracy: 0.7206 - val_loss: 0.4963 - val_accuracy: 0.7222\n",
      "Epoch 692/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5552 - accuracy: 0.7206 - val_loss: 0.4899 - val_accuracy: 0.7222\n",
      "Epoch 693/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5567 - accuracy: 0.7059 - val_loss: 0.4950 - val_accuracy: 0.7222\n",
      "Epoch 694/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5544 - accuracy: 0.7059 - val_loss: 0.5118 - val_accuracy: 0.7222\n",
      "Epoch 695/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5522 - accuracy: 0.7059 - val_loss: 0.4936 - val_accuracy: 0.7778\n",
      "Epoch 696/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5516 - accuracy: 0.7206 - val_loss: 0.4627 - val_accuracy: 0.7778\n",
      "Epoch 697/1000\n",
      "68/68 [==============================] - 0s 277us/step - loss: 0.5621 - accuracy: 0.7206 - val_loss: 0.4520 - val_accuracy: 0.7778\n",
      "Epoch 698/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5621 - accuracy: 0.7059 - val_loss: 0.4649 - val_accuracy: 0.7778\n",
      "Epoch 699/1000\n",
      "68/68 [==============================] - 0s 189us/step - loss: 0.5509 - accuracy: 0.7206 - val_loss: 0.5007 - val_accuracy: 0.7222\n",
      "Epoch 700/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5520 - accuracy: 0.7059 - val_loss: 0.5215 - val_accuracy: 0.7222\n",
      "Epoch 701/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5543 - accuracy: 0.7206 - val_loss: 0.5177 - val_accuracy: 0.7222\n",
      "Epoch 702/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5520 - accuracy: 0.7059 - val_loss: 0.5045 - val_accuracy: 0.7222\n",
      "Epoch 703/1000\n",
      "68/68 [==============================] - 0s 168us/step - loss: 0.5502 - accuracy: 0.7059 - val_loss: 0.5006 - val_accuracy: 0.7222\n",
      "Epoch 704/1000\n",
      "68/68 [==============================] - 0s 177us/step - loss: 0.5503 - accuracy: 0.7206 - val_loss: 0.4964 - val_accuracy: 0.7778\n",
      "Epoch 705/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5507 - accuracy: 0.7206 - val_loss: 0.4971 - val_accuracy: 0.7778\n",
      "Epoch 706/1000\n",
      "68/68 [==============================] - 0s 154us/step - loss: 0.5533 - accuracy: 0.7353 - val_loss: 0.4879 - val_accuracy: 0.7778\n",
      "Epoch 707/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5540 - accuracy: 0.7353 - val_loss: 0.4914 - val_accuracy: 0.7778\n",
      "Epoch 708/1000\n",
      "68/68 [==============================] - 0s 156us/step - loss: 0.5546 - accuracy: 0.7353 - val_loss: 0.5042 - val_accuracy: 0.7778\n",
      "Epoch 709/1000\n",
      "68/68 [==============================] - 0s 181us/step - loss: 0.5473 - accuracy: 0.7353 - val_loss: 0.5024 - val_accuracy: 0.7778\n",
      "Epoch 710/1000\n",
      "68/68 [==============================] - 0s 137us/step - loss: 0.5496 - accuracy: 0.7353 - val_loss: 0.5202 - val_accuracy: 0.7222\n",
      "Epoch 711/1000\n",
      "68/68 [==============================] - 0s 128us/step - loss: 0.5643 - accuracy: 0.7059 - val_loss: 0.5685 - val_accuracy: 0.6667\n",
      "Epoch 712/1000\n",
      "68/68 [==============================] - 0s 254us/step - loss: 0.5780 - accuracy: 0.6912 - val_loss: 0.5583 - val_accuracy: 0.6667\n",
      "Epoch 713/1000\n",
      "68/68 [==============================] - 0s 173us/step - loss: 0.5658 - accuracy: 0.7059 - val_loss: 0.5085 - val_accuracy: 0.7222\n",
      "Epoch 714/1000\n",
      "68/68 [==============================] - 0s 0us/step - loss: 0.5486 - accuracy: 0.6912 - val_loss: 0.4719 - val_accuracy: 0.8333\n",
      "Epoch 715/1000\n",
      "68/68 [==============================] - 0s 338us/step - loss: 0.5442 - accuracy: 0.6765 - val_loss: 0.4635 - val_accuracy: 0.8333\n",
      "Epoch 716/1000\n",
      "68/68 [==============================] - 0s 187us/step - loss: 0.5471 - accuracy: 0.7059 - val_loss: 0.4694 - val_accuracy: 0.7778\n",
      "Epoch 717/1000\n",
      "68/68 [==============================] - 0s 171us/step - loss: 0.5484 - accuracy: 0.7353 - val_loss: 0.4743 - val_accuracy: 0.7778\n",
      "Epoch 718/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.5475 - accuracy: 0.7206 - val_loss: 0.4714 - val_accuracy: 0.7778\n",
      "Epoch 719/1000\n",
      "68/68 [==============================] - 0s 164us/step - loss: 0.5478 - accuracy: 0.7206 - val_loss: 0.4847 - val_accuracy: 0.7778\n",
      "Epoch 720/1000\n",
      "68/68 [==============================] - 0s 206us/step - loss: 0.5491 - accuracy: 0.7353 - val_loss: 0.4987 - val_accuracy: 0.7778\n",
      "Epoch 721/1000\n",
      "68/68 [==============================] - 0s 212us/step - loss: 0.5505 - accuracy: 0.7353 - val_loss: 0.4776 - val_accuracy: 0.7778\n",
      "Epoch 722/1000\n",
      "68/68 [==============================] - 0s 173us/step - loss: 0.5464 - accuracy: 0.7353 - val_loss: 0.4905 - val_accuracy: 0.7778\n",
      "Epoch 723/1000\n",
      "68/68 [==============================] - 0s 149us/step - loss: 0.5443 - accuracy: 0.7353 - val_loss: 0.5045 - val_accuracy: 0.7778\n",
      "Epoch 724/1000\n",
      "68/68 [==============================] - 0s 137us/step - loss: 0.5486 - accuracy: 0.7353 - val_loss: 0.4967 - val_accuracy: 0.7778\n",
      "Epoch 725/1000\n",
      "68/68 [==============================] - 0s 153us/step - loss: 0.5478 - accuracy: 0.7353 - val_loss: 0.4692 - val_accuracy: 0.7778\n",
      "Epoch 726/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 147us/step - loss: 0.5496 - accuracy: 0.7206 - val_loss: 0.4686 - val_accuracy: 0.7778\n",
      "Epoch 727/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5499 - accuracy: 0.7206 - val_loss: 0.4847 - val_accuracy: 0.7778\n",
      "Epoch 728/1000\n",
      "68/68 [==============================] - 0s 177us/step - loss: 0.5499 - accuracy: 0.7206 - val_loss: 0.4980 - val_accuracy: 0.7778\n",
      "Epoch 729/1000\n",
      "68/68 [==============================] - 0s 181us/step - loss: 0.5504 - accuracy: 0.7206 - val_loss: 0.5034 - val_accuracy: 0.7778\n",
      "Epoch 730/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5551 - accuracy: 0.7206 - val_loss: 0.5221 - val_accuracy: 0.7222\n",
      "Epoch 731/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5605 - accuracy: 0.7059 - val_loss: 0.5191 - val_accuracy: 0.7222\n",
      "Epoch 732/1000\n",
      "68/68 [==============================] - 0s 163us/step - loss: 0.5594 - accuracy: 0.7059 - val_loss: 0.4838 - val_accuracy: 0.8333\n",
      "Epoch 733/1000\n",
      "68/68 [==============================] - 0s 165us/step - loss: 0.5497 - accuracy: 0.7059 - val_loss: 0.4555 - val_accuracy: 0.8333\n",
      "Epoch 734/1000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.5631 - accuracy: 0.68 - 0s 178us/step - loss: 0.5560 - accuracy: 0.6912 - val_loss: 0.4557 - val_accuracy: 0.8333\n",
      "Epoch 735/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5560 - accuracy: 0.7059 - val_loss: 0.4700 - val_accuracy: 0.8333\n",
      "Epoch 736/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5539 - accuracy: 0.6912 - val_loss: 0.5186 - val_accuracy: 0.7222\n",
      "Epoch 737/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5679 - accuracy: 0.7059 - val_loss: 0.5373 - val_accuracy: 0.7222\n",
      "Epoch 738/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5657 - accuracy: 0.7059 - val_loss: 0.4977 - val_accuracy: 0.7778\n",
      "Epoch 739/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5529 - accuracy: 0.6765 - val_loss: 0.4688 - val_accuracy: 0.8333\n",
      "Epoch 740/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5493 - accuracy: 0.6912 - val_loss: 0.4569 - val_accuracy: 0.8333\n",
      "Epoch 741/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5491 - accuracy: 0.7059 - val_loss: 0.4515 - val_accuracy: 0.8333\n",
      "Epoch 742/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5511 - accuracy: 0.7059 - val_loss: 0.4553 - val_accuracy: 0.8333\n",
      "Epoch 743/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5479 - accuracy: 0.6912 - val_loss: 0.4650 - val_accuracy: 0.8333\n",
      "Epoch 744/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5517 - accuracy: 0.7059 - val_loss: 0.4857 - val_accuracy: 0.7778\n",
      "Epoch 745/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.5432 - accuracy: 0.7059 - val_loss: 0.4898 - val_accuracy: 0.7778\n",
      "Epoch 746/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5455 - accuracy: 0.7206 - val_loss: 0.4798 - val_accuracy: 0.7778\n",
      "Epoch 747/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5389 - accuracy: 0.7206 - val_loss: 0.4461 - val_accuracy: 0.8333\n",
      "Epoch 748/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5698 - accuracy: 0.6912 - val_loss: 0.4373 - val_accuracy: 0.8333\n",
      "Epoch 749/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.6119 - accuracy: 0.6912 - val_loss: 0.4461 - val_accuracy: 0.8333\n",
      "Epoch 750/1000\n",
      "68/68 [==============================] - 0s 177us/step - loss: 0.5566 - accuracy: 0.7206 - val_loss: 0.5158 - val_accuracy: 0.7222\n",
      "Epoch 751/1000\n",
      "68/68 [==============================] - 0s 155us/step - loss: 0.5662 - accuracy: 0.7206 - val_loss: 0.6545 - val_accuracy: 0.6111\n",
      "Epoch 752/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.6194 - accuracy: 0.7353 - val_loss: 0.6702 - val_accuracy: 0.6111\n",
      "Epoch 753/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.6073 - accuracy: 0.7500 - val_loss: 0.5685 - val_accuracy: 0.6667\n",
      "Epoch 754/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5625 - accuracy: 0.7059 - val_loss: 0.5082 - val_accuracy: 0.7778\n",
      "Epoch 755/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5494 - accuracy: 0.7206 - val_loss: 0.5026 - val_accuracy: 0.7778\n",
      "Epoch 756/1000\n",
      "68/68 [==============================] - 0s 163us/step - loss: 0.5496 - accuracy: 0.7059 - val_loss: 0.4976 - val_accuracy: 0.7778\n",
      "Epoch 757/1000\n",
      "68/68 [==============================] - 0s 157us/step - loss: 0.5479 - accuracy: 0.7059 - val_loss: 0.4889 - val_accuracy: 0.7778\n",
      "Epoch 758/1000\n",
      "68/68 [==============================] - 0s 170us/step - loss: 0.5446 - accuracy: 0.7353 - val_loss: 0.4909 - val_accuracy: 0.7778\n",
      "Epoch 759/1000\n",
      "68/68 [==============================] - 0s 160us/step - loss: 0.5434 - accuracy: 0.7353 - val_loss: 0.4876 - val_accuracy: 0.7778\n",
      "Epoch 760/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.5418 - accuracy: 0.7353 - val_loss: 0.4918 - val_accuracy: 0.7778\n",
      "Epoch 761/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5426 - accuracy: 0.7353 - val_loss: 0.5059 - val_accuracy: 0.7222\n",
      "Epoch 762/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5445 - accuracy: 0.7353 - val_loss: 0.5176 - val_accuracy: 0.7222\n",
      "Epoch 763/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.5455 - accuracy: 0.7353 - val_loss: 0.5176 - val_accuracy: 0.7222\n",
      "Epoch 764/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5515 - accuracy: 0.7059 - val_loss: 0.5530 - val_accuracy: 0.6667\n",
      "Epoch 765/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5598 - accuracy: 0.6912 - val_loss: 0.5522 - val_accuracy: 0.6667\n",
      "Epoch 766/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5614 - accuracy: 0.7059 - val_loss: 0.5236 - val_accuracy: 0.7222\n",
      "Epoch 767/1000\n",
      "68/68 [==============================] - 0s 177us/step - loss: 0.5540 - accuracy: 0.7059 - val_loss: 0.4594 - val_accuracy: 0.8333\n",
      "Epoch 768/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5519 - accuracy: 0.7059 - val_loss: 0.4319 - val_accuracy: 0.7222\n",
      "Epoch 769/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5948 - accuracy: 0.6912 - val_loss: 0.4271 - val_accuracy: 0.7222\n",
      "Epoch 770/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5845 - accuracy: 0.7059 - val_loss: 0.4393 - val_accuracy: 0.8333\n",
      "Epoch 771/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5500 - accuracy: 0.6765 - val_loss: 0.5116 - val_accuracy: 0.7222\n",
      "Epoch 772/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5588 - accuracy: 0.7059 - val_loss: 0.5834 - val_accuracy: 0.6667\n",
      "Epoch 773/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5867 - accuracy: 0.7059 - val_loss: 0.5855 - val_accuracy: 0.6667\n",
      "Epoch 774/1000\n",
      "68/68 [==============================] - 0s 151us/step - loss: 0.5752 - accuracy: 0.7059 - val_loss: 0.5141 - val_accuracy: 0.7222\n",
      "Epoch 775/1000\n",
      "68/68 [==============================] - 0s 163us/step - loss: 0.5487 - accuracy: 0.7206 - val_loss: 0.4700 - val_accuracy: 0.7778\n",
      "Epoch 776/1000\n",
      "68/68 [==============================] - 0s 198us/step - loss: 0.5393 - accuracy: 0.7059 - val_loss: 0.4625 - val_accuracy: 0.8333\n",
      "Epoch 777/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5409 - accuracy: 0.7206 - val_loss: 0.4532 - val_accuracy: 0.8333\n",
      "Epoch 778/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5457 - accuracy: 0.7059 - val_loss: 0.4517 - val_accuracy: 0.8333\n",
      "Epoch 779/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5466 - accuracy: 0.7059 - val_loss: 0.4563 - val_accuracy: 0.8333\n",
      "Epoch 780/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5406 - accuracy: 0.6912 - val_loss: 0.4827 - val_accuracy: 0.7778\n",
      "Epoch 781/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5386 - accuracy: 0.7353 - val_loss: 0.5395 - val_accuracy: 0.7222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 782/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5569 - accuracy: 0.7059 - val_loss: 0.5662 - val_accuracy: 0.6111\n",
      "Epoch 783/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.5714 - accuracy: 0.6912 - val_loss: 0.5689 - val_accuracy: 0.6667\n",
      "Epoch 784/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.5712 - accuracy: 0.6912 - val_loss: 0.4929 - val_accuracy: 0.7778\n",
      "Epoch 785/1000\n",
      "68/68 [==============================] - 0s 221us/step - loss: 0.5503 - accuracy: 0.6912 - val_loss: 0.4547 - val_accuracy: 0.8333\n",
      "Epoch 786/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.5546 - accuracy: 0.7059 - val_loss: 0.4494 - val_accuracy: 0.7778\n",
      "Epoch 787/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5627 - accuracy: 0.7206 - val_loss: 0.4579 - val_accuracy: 0.8333\n",
      "Epoch 788/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.5455 - accuracy: 0.7206 - val_loss: 0.4986 - val_accuracy: 0.7778\n",
      "Epoch 789/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.5408 - accuracy: 0.7206 - val_loss: 0.5575 - val_accuracy: 0.6667\n",
      "Epoch 790/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5635 - accuracy: 0.7206 - val_loss: 0.5916 - val_accuracy: 0.6667\n",
      "Epoch 791/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5712 - accuracy: 0.7206 - val_loss: 0.5529 - val_accuracy: 0.6667\n",
      "Epoch 792/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5503 - accuracy: 0.7206 - val_loss: 0.4996 - val_accuracy: 0.7222\n",
      "Epoch 793/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5404 - accuracy: 0.7353 - val_loss: 0.4682 - val_accuracy: 0.7778\n",
      "Epoch 794/1000\n",
      "68/68 [==============================] - 0s 184us/step - loss: 0.5454 - accuracy: 0.7500 - val_loss: 0.4581 - val_accuracy: 0.8333\n",
      "Epoch 795/1000\n",
      "68/68 [==============================] - 0s 189us/step - loss: 0.5431 - accuracy: 0.7206 - val_loss: 0.4689 - val_accuracy: 0.7778\n",
      "Epoch 796/1000\n",
      "68/68 [==============================] - 0s 107us/step - loss: 0.5360 - accuracy: 0.7206 - val_loss: 0.4876 - val_accuracy: 0.7778\n",
      "Epoch 797/1000\n",
      "68/68 [==============================] - 0s 248us/step - loss: 0.5407 - accuracy: 0.7206 - val_loss: 0.4935 - val_accuracy: 0.7778\n",
      "Epoch 798/1000\n",
      "68/68 [==============================] - 0s 160us/step - loss: 0.5383 - accuracy: 0.7206 - val_loss: 0.4828 - val_accuracy: 0.7778\n",
      "Epoch 799/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.5354 - accuracy: 0.7059 - val_loss: 0.4738 - val_accuracy: 0.8333\n",
      "Epoch 800/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5373 - accuracy: 0.7059 - val_loss: 0.4653 - val_accuracy: 0.8333\n",
      "Epoch 801/1000\n",
      "68/68 [==============================] - 0s 100us/step - loss: 0.5427 - accuracy: 0.7059 - val_loss: 0.4659 - val_accuracy: 0.8333\n",
      "Epoch 802/1000\n",
      "68/68 [==============================] - 0s 229us/step - loss: 0.5416 - accuracy: 0.7059 - val_loss: 0.4843 - val_accuracy: 0.7778\n",
      "Epoch 803/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5437 - accuracy: 0.7353 - val_loss: 0.5013 - val_accuracy: 0.7778\n",
      "Epoch 804/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5422 - accuracy: 0.7353 - val_loss: 0.4945 - val_accuracy: 0.7778\n",
      "Epoch 805/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5427 - accuracy: 0.7500 - val_loss: 0.4851 - val_accuracy: 0.7778\n",
      "Epoch 806/1000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.5341 - accuracy: 0.75 - 0s 162us/step - loss: 0.5488 - accuracy: 0.7059 - val_loss: 0.4673 - val_accuracy: 0.8333\n",
      "Epoch 807/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5579 - accuracy: 0.7206 - val_loss: 0.4680 - val_accuracy: 0.8333\n",
      "Epoch 808/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5563 - accuracy: 0.7353 - val_loss: 0.4738 - val_accuracy: 0.7778\n",
      "Epoch 809/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5451 - accuracy: 0.7500 - val_loss: 0.4832 - val_accuracy: 0.7778\n",
      "Epoch 810/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5402 - accuracy: 0.7353 - val_loss: 0.5104 - val_accuracy: 0.7222\n",
      "Epoch 811/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5427 - accuracy: 0.7353 - val_loss: 0.5230 - val_accuracy: 0.7222\n",
      "Epoch 812/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5436 - accuracy: 0.7353 - val_loss: 0.5191 - val_accuracy: 0.7222\n",
      "Epoch 813/1000\n",
      "68/68 [==============================] - 0s 179us/step - loss: 0.5446 - accuracy: 0.7353 - val_loss: 0.5071 - val_accuracy: 0.7222\n",
      "Epoch 814/1000\n",
      "68/68 [==============================] - 0s 175us/step - loss: 0.5436 - accuracy: 0.7206 - val_loss: 0.5062 - val_accuracy: 0.7222\n",
      "Epoch 815/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5447 - accuracy: 0.7206 - val_loss: 0.5109 - val_accuracy: 0.7222\n",
      "Epoch 816/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.5449 - accuracy: 0.7353 - val_loss: 0.5051 - val_accuracy: 0.7222\n",
      "Epoch 817/1000\n",
      "68/68 [==============================] - 0s 165us/step - loss: 0.5421 - accuracy: 0.7206 - val_loss: 0.4911 - val_accuracy: 0.7222\n",
      "Epoch 818/1000\n",
      "68/68 [==============================] - 0s 166us/step - loss: 0.5444 - accuracy: 0.7353 - val_loss: 0.4727 - val_accuracy: 0.7778\n",
      "Epoch 819/1000\n",
      "68/68 [==============================] - 0s 86us/step - loss: 0.5371 - accuracy: 0.7353 - val_loss: 0.4764 - val_accuracy: 0.7778\n",
      "Epoch 820/1000\n",
      "68/68 [==============================] - 0s 249us/step - loss: 0.5325 - accuracy: 0.7353 - val_loss: 0.4903 - val_accuracy: 0.7778\n",
      "Epoch 821/1000\n",
      "68/68 [==============================] - 0s 83us/step - loss: 0.5356 - accuracy: 0.7353 - val_loss: 0.4740 - val_accuracy: 0.7778\n",
      "Epoch 822/1000\n",
      "68/68 [==============================] - 0s 235us/step - loss: 0.5400 - accuracy: 0.6912 - val_loss: 0.4387 - val_accuracy: 0.8333\n",
      "Epoch 823/1000\n",
      "68/68 [==============================] - 0s 99us/step - loss: 0.5555 - accuracy: 0.7059 - val_loss: 0.4356 - val_accuracy: 0.8333\n",
      "Epoch 824/1000\n",
      "68/68 [==============================] - 0s 293us/step - loss: 0.5640 - accuracy: 0.7059 - val_loss: 0.4397 - val_accuracy: 0.8333\n",
      "Epoch 825/1000\n",
      "68/68 [==============================] - 0s 141us/step - loss: 0.5585 - accuracy: 0.7206 - val_loss: 0.4471 - val_accuracy: 0.8333\n",
      "Epoch 826/1000\n",
      "68/68 [==============================] - 0s 159us/step - loss: 0.5493 - accuracy: 0.7059 - val_loss: 0.4606 - val_accuracy: 0.8333\n",
      "Epoch 827/1000\n",
      "68/68 [==============================] - 0s 144us/step - loss: 0.5418 - accuracy: 0.6912 - val_loss: 0.4749 - val_accuracy: 0.8333\n",
      "Epoch 828/1000\n",
      "68/68 [==============================] - 0s 144us/step - loss: 0.5393 - accuracy: 0.6765 - val_loss: 0.4948 - val_accuracy: 0.7222\n",
      "Epoch 829/1000\n",
      "68/68 [==============================] - 0s 106us/step - loss: 0.5464 - accuracy: 0.6912 - val_loss: 0.4897 - val_accuracy: 0.7778\n",
      "Epoch 830/1000\n",
      "68/68 [==============================] - 0s 250us/step - loss: 0.5329 - accuracy: 0.7059 - val_loss: 0.4487 - val_accuracy: 0.8333\n",
      "Epoch 831/1000\n",
      "68/68 [==============================] - 0s 143us/step - loss: 0.5369 - accuracy: 0.7206 - val_loss: 0.4441 - val_accuracy: 0.8333\n",
      "Epoch 832/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5442 - accuracy: 0.7059 - val_loss: 0.4507 - val_accuracy: 0.8333\n",
      "Epoch 833/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5402 - accuracy: 0.7059 - val_loss: 0.4613 - val_accuracy: 0.8333\n",
      "Epoch 834/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.5350 - accuracy: 0.7206 - val_loss: 0.4646 - val_accuracy: 0.8333\n",
      "Epoch 835/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5308 - accuracy: 0.7059 - val_loss: 0.4606 - val_accuracy: 0.8333\n",
      "Epoch 836/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5334 - accuracy: 0.7059 - val_loss: 0.4617 - val_accuracy: 0.8333\n",
      "Epoch 837/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5320 - accuracy: 0.7059 - val_loss: 0.4836 - val_accuracy: 0.7778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 838/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5403 - accuracy: 0.6765 - val_loss: 0.5289 - val_accuracy: 0.7222\n",
      "Epoch 839/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5536 - accuracy: 0.6912 - val_loss: 0.5291 - val_accuracy: 0.7222\n",
      "Epoch 840/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5490 - accuracy: 0.7206 - val_loss: 0.5054 - val_accuracy: 0.7222\n",
      "Epoch 841/1000\n",
      "68/68 [==============================] - 0s 131us/step - loss: 0.5366 - accuracy: 0.7059 - val_loss: 0.4760 - val_accuracy: 0.8333\n",
      "Epoch 842/1000\n",
      "68/68 [==============================] - 0s 140us/step - loss: 0.5341 - accuracy: 0.6912 - val_loss: 0.4621 - val_accuracy: 0.8333\n",
      "Epoch 843/1000\n",
      "68/68 [==============================] - 0s 101us/step - loss: 0.5348 - accuracy: 0.7059 - val_loss: 0.4540 - val_accuracy: 0.8333\n",
      "Epoch 844/1000\n",
      "68/68 [==============================] - 0s 243us/step - loss: 0.5412 - accuracy: 0.7059 - val_loss: 0.4424 - val_accuracy: 0.8333\n",
      "Epoch 845/1000\n",
      "68/68 [==============================] - 0s 177us/step - loss: 0.5430 - accuracy: 0.7059 - val_loss: 0.4577 - val_accuracy: 0.8333\n",
      "Epoch 846/1000\n",
      "68/68 [==============================] - 0s 168us/step - loss: 0.5297 - accuracy: 0.7206 - val_loss: 0.4998 - val_accuracy: 0.7778\n",
      "Epoch 847/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5341 - accuracy: 0.7059 - val_loss: 0.5214 - val_accuracy: 0.6667\n",
      "Epoch 848/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5421 - accuracy: 0.7059 - val_loss: 0.5170 - val_accuracy: 0.7222\n",
      "Epoch 849/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.5406 - accuracy: 0.7059 - val_loss: 0.5025 - val_accuracy: 0.7778\n",
      "Epoch 850/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5361 - accuracy: 0.7059 - val_loss: 0.4855 - val_accuracy: 0.7778\n",
      "Epoch 851/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5348 - accuracy: 0.7206 - val_loss: 0.4765 - val_accuracy: 0.7778\n",
      "Epoch 852/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.5312 - accuracy: 0.7059 - val_loss: 0.4529 - val_accuracy: 0.8333\n",
      "Epoch 853/1000\n",
      "68/68 [==============================] - 0s 133us/step - loss: 0.5332 - accuracy: 0.6765 - val_loss: 0.4440 - val_accuracy: 0.8333\n",
      "Epoch 854/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5352 - accuracy: 0.6912 - val_loss: 0.4461 - val_accuracy: 0.8333\n",
      "Epoch 855/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5334 - accuracy: 0.6912 - val_loss: 0.4499 - val_accuracy: 0.8333\n",
      "Epoch 856/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5337 - accuracy: 0.6765 - val_loss: 0.4406 - val_accuracy: 0.8333\n",
      "Epoch 857/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5359 - accuracy: 0.7059 - val_loss: 0.4416 - val_accuracy: 0.8333\n",
      "Epoch 858/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5347 - accuracy: 0.7059 - val_loss: 0.4514 - val_accuracy: 0.8333\n",
      "Epoch 859/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5316 - accuracy: 0.7059 - val_loss: 0.4748 - val_accuracy: 0.8333\n",
      "Epoch 860/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5380 - accuracy: 0.6912 - val_loss: 0.4849 - val_accuracy: 0.8333\n",
      "Epoch 861/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5419 - accuracy: 0.6912 - val_loss: 0.4836 - val_accuracy: 0.7778\n",
      "Epoch 862/1000\n",
      "68/68 [==============================] - 0s 140us/step - loss: 0.5405 - accuracy: 0.6765 - val_loss: 0.4616 - val_accuracy: 0.8333\n",
      "Epoch 863/1000\n",
      "68/68 [==============================] - 0s 144us/step - loss: 0.5456 - accuracy: 0.7059 - val_loss: 0.4446 - val_accuracy: 0.7778\n",
      "Epoch 864/1000\n",
      "68/68 [==============================] - 0s 229us/step - loss: 0.5572 - accuracy: 0.7059 - val_loss: 0.4402 - val_accuracy: 0.7778\n",
      "Epoch 865/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.5517 - accuracy: 0.7206 - val_loss: 0.4533 - val_accuracy: 0.8333\n",
      "Epoch 866/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5359 - accuracy: 0.6912 - val_loss: 0.5107 - val_accuracy: 0.7222\n",
      "Epoch 867/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5437 - accuracy: 0.7206 - val_loss: 0.5659 - val_accuracy: 0.6667\n",
      "Epoch 868/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5583 - accuracy: 0.7206 - val_loss: 0.5590 - val_accuracy: 0.6667\n",
      "Epoch 869/1000\n",
      "68/68 [==============================] - 0s 160us/step - loss: 0.5507 - accuracy: 0.7206 - val_loss: 0.5004 - val_accuracy: 0.7778\n",
      "Epoch 870/1000\n",
      "68/68 [==============================] - 0s 169us/step - loss: 0.5340 - accuracy: 0.7353 - val_loss: 0.4541 - val_accuracy: 0.8333\n",
      "Epoch 871/1000\n",
      "68/68 [==============================] - 0s 110us/step - loss: 0.5393 - accuracy: 0.7206 - val_loss: 0.4435 - val_accuracy: 0.8333\n",
      "Epoch 872/1000\n",
      "68/68 [==============================] - 0s 306us/step - loss: 0.5392 - accuracy: 0.7059 - val_loss: 0.4526 - val_accuracy: 0.8333\n",
      "Epoch 873/1000\n",
      "68/68 [==============================] - 0s 174us/step - loss: 0.5282 - accuracy: 0.7206 - val_loss: 0.4766 - val_accuracy: 0.7778\n",
      "Epoch 874/1000\n",
      "68/68 [==============================] - 0s 169us/step - loss: 0.5245 - accuracy: 0.7206 - val_loss: 0.5016 - val_accuracy: 0.7778\n",
      "Epoch 875/1000\n",
      "68/68 [==============================] - 0s 168us/step - loss: 0.5307 - accuracy: 0.7206 - val_loss: 0.5257 - val_accuracy: 0.6667\n",
      "Epoch 876/1000\n",
      "68/68 [==============================] - 0s 182us/step - loss: 0.5416 - accuracy: 0.6912 - val_loss: 0.5453 - val_accuracy: 0.6667\n",
      "Epoch 877/1000\n",
      "68/68 [==============================] - 0s 164us/step - loss: 0.5535 - accuracy: 0.7059 - val_loss: 0.5593 - val_accuracy: 0.6667\n",
      "Epoch 878/1000\n",
      "68/68 [==============================] - 0s 160us/step - loss: 0.5535 - accuracy: 0.7059 - val_loss: 0.5097 - val_accuracy: 0.7222\n",
      "Epoch 879/1000\n",
      "68/68 [==============================] - 0s 159us/step - loss: 0.5377 - accuracy: 0.7059 - val_loss: 0.4623 - val_accuracy: 0.8333\n",
      "Epoch 880/1000\n",
      "68/68 [==============================] - 0s 208us/step - loss: 0.5236 - accuracy: 0.7059 - val_loss: 0.4448 - val_accuracy: 0.8333\n",
      "Epoch 881/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5412 - accuracy: 0.7059 - val_loss: 0.4402 - val_accuracy: 0.8333\n",
      "Epoch 882/1000\n",
      "68/68 [==============================] - 0s 103us/step - loss: 0.5642 - accuracy: 0.7206 - val_loss: 0.4421 - val_accuracy: 0.8333\n",
      "Epoch 883/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.5544 - accuracy: 0.7206 - val_loss: 0.4614 - val_accuracy: 0.8333\n",
      "Epoch 884/1000\n",
      "68/68 [==============================] - 0s 103us/step - loss: 0.5301 - accuracy: 0.7059 - val_loss: 0.5045 - val_accuracy: 0.7222\n",
      "Epoch 885/1000\n",
      "68/68 [==============================] - 0s 103us/step - loss: 0.5438 - accuracy: 0.7206 - val_loss: 0.5364 - val_accuracy: 0.7222\n",
      "Epoch 886/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5459 - accuracy: 0.7059 - val_loss: 0.5019 - val_accuracy: 0.7222\n",
      "Epoch 887/1000\n",
      "68/68 [==============================] - 0s 103us/step - loss: 0.5374 - accuracy: 0.7206 - val_loss: 0.4546 - val_accuracy: 0.8333\n",
      "Epoch 888/1000\n",
      "68/68 [==============================] - 0s 45us/step - loss: 0.5381 - accuracy: 0.6912 - val_loss: 0.4339 - val_accuracy: 0.8333\n",
      "Epoch 889/1000\n",
      "68/68 [==============================] - 0s 0us/step - loss: 0.5717 - accuracy: 0.7206 - val_loss: 0.4311 - val_accuracy: 0.7778\n",
      "Epoch 890/1000\n",
      "68/68 [==============================] - 0s 0us/step - loss: 0.5788 - accuracy: 0.7059 - val_loss: 0.4316 - val_accuracy: 0.8333\n",
      "Epoch 891/1000\n",
      "68/68 [==============================] - 0s 230us/step - loss: 0.5529 - accuracy: 0.7059 - val_loss: 0.4454 - val_accuracy: 0.8333\n",
      "Epoch 892/1000\n",
      "68/68 [==============================] - 0s 0us/step - loss: 0.5345 - accuracy: 0.6912 - val_loss: 0.4814 - val_accuracy: 0.7778\n",
      "Epoch 893/1000\n",
      "68/68 [==============================] - 0s 230us/step - loss: 0.5305 - accuracy: 0.7059 - val_loss: 0.5045 - val_accuracy: 0.7222\n",
      "Epoch 894/1000\n",
      "68/68 [==============================] - 0s 0us/step - loss: 0.5331 - accuracy: 0.7059 - val_loss: 0.5098 - val_accuracy: 0.7222\n",
      "Epoch 895/1000\n",
      "68/68 [==============================] - 0s 0us/step - loss: 0.5382 - accuracy: 0.7059 - val_loss: 0.5090 - val_accuracy: 0.7222\n",
      "Epoch 896/1000\n",
      "68/68 [==============================] - 0s 318us/step - loss: 0.5371 - accuracy: 0.7059 - val_loss: 0.5013 - val_accuracy: 0.7222\n",
      "Epoch 897/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.5323 - accuracy: 0.7059 - val_loss: 0.5113 - val_accuracy: 0.7222\n",
      "Epoch 898/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5318 - accuracy: 0.7206 - val_loss: 0.5013 - val_accuracy: 0.7222\n",
      "Epoch 899/1000\n",
      "68/68 [==============================] - 0s 177us/step - loss: 0.5295 - accuracy: 0.7353 - val_loss: 0.4692 - val_accuracy: 0.7778\n",
      "Epoch 900/1000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.5766 - accuracy: 0.81 - 0s 132us/step - loss: 0.5600 - accuracy: 0.7353 - val_loss: 0.4781 - val_accuracy: 0.7778\n",
      "Epoch 901/1000\n",
      "68/68 [==============================] - 0s 206us/step - loss: 0.5283 - accuracy: 0.7353 - val_loss: 0.4742 - val_accuracy: 0.7778\n",
      "Epoch 902/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.5264 - accuracy: 0.7353 - val_loss: 0.4874 - val_accuracy: 0.7778\n",
      "Epoch 903/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.5290 - accuracy: 0.7353 - val_loss: 0.5079 - val_accuracy: 0.7778\n",
      "Epoch 904/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5310 - accuracy: 0.7206 - val_loss: 0.5051 - val_accuracy: 0.7778\n",
      "Epoch 905/1000\n",
      "68/68 [==============================] - 0s 250us/step - loss: 0.5299 - accuracy: 0.7206 - val_loss: 0.4905 - val_accuracy: 0.7778\n",
      "Epoch 906/1000\n",
      "68/68 [==============================] - 0s 206us/step - loss: 0.5275 - accuracy: 0.7353 - val_loss: 0.4668 - val_accuracy: 0.7778\n",
      "Epoch 907/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.5262 - accuracy: 0.7206 - val_loss: 0.4473 - val_accuracy: 0.8333\n",
      "Epoch 908/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5384 - accuracy: 0.7206 - val_loss: 0.4413 - val_accuracy: 0.8333\n",
      "Epoch 909/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5521 - accuracy: 0.7500 - val_loss: 0.4439 - val_accuracy: 0.8333\n",
      "Epoch 910/1000\n",
      "68/68 [==============================] - 0s 132us/step - loss: 0.5407 - accuracy: 0.7353 - val_loss: 0.4607 - val_accuracy: 0.7778\n",
      "Epoch 911/1000\n",
      "68/68 [==============================] - 0s 152us/step - loss: 0.5265 - accuracy: 0.7353 - val_loss: 0.5000 - val_accuracy: 0.7222\n",
      "Epoch 912/1000\n",
      "68/68 [==============================] - 0s 126us/step - loss: 0.5281 - accuracy: 0.7353 - val_loss: 0.5173 - val_accuracy: 0.6667\n",
      "Epoch 913/1000\n",
      "68/68 [==============================] - 0s 135us/step - loss: 0.5359 - accuracy: 0.6912 - val_loss: 0.5128 - val_accuracy: 0.7222\n",
      "Epoch 914/1000\n",
      "68/68 [==============================] - 0s 128us/step - loss: 0.5327 - accuracy: 0.7059 - val_loss: 0.4891 - val_accuracy: 0.7778\n",
      "Epoch 915/1000\n",
      "68/68 [==============================] - 0s 190us/step - loss: 0.5253 - accuracy: 0.7353 - val_loss: 0.4770 - val_accuracy: 0.7778\n",
      "Epoch 916/1000\n",
      "68/68 [==============================] - 0s 151us/step - loss: 0.5228 - accuracy: 0.7353 - val_loss: 0.4597 - val_accuracy: 0.7778\n",
      "Epoch 917/1000\n",
      "68/68 [==============================] - 0s 167us/step - loss: 0.5252 - accuracy: 0.7206 - val_loss: 0.4382 - val_accuracy: 0.8333\n",
      "Epoch 918/1000\n",
      "68/68 [==============================] - 0s 170us/step - loss: 0.5286 - accuracy: 0.7059 - val_loss: 0.4310 - val_accuracy: 0.8333\n",
      "Epoch 919/1000\n",
      "68/68 [==============================] - 0s 167us/step - loss: 0.5296 - accuracy: 0.7206 - val_loss: 0.4406 - val_accuracy: 0.8333\n",
      "Epoch 920/1000\n",
      "68/68 [==============================] - 0s 158us/step - loss: 0.5252 - accuracy: 0.6912 - val_loss: 0.4600 - val_accuracy: 0.8333\n",
      "Epoch 921/1000\n",
      "68/68 [==============================] - 0s 0us/step - loss: 0.5246 - accuracy: 0.6912 - val_loss: 0.4721 - val_accuracy: 0.7778\n",
      "Epoch 922/1000\n",
      "68/68 [==============================] - 0s 134us/step - loss: 0.5241 - accuracy: 0.7206 - val_loss: 0.4740 - val_accuracy: 0.7778\n",
      "Epoch 923/1000\n",
      "68/68 [==============================] - 0s 170us/step - loss: 0.5234 - accuracy: 0.7206 - val_loss: 0.4688 - val_accuracy: 0.7778\n",
      "Epoch 924/1000\n",
      "68/68 [==============================] - 0s 123us/step - loss: 0.5227 - accuracy: 0.7206 - val_loss: 0.4695 - val_accuracy: 0.7778\n",
      "Epoch 925/1000\n",
      "68/68 [==============================] - 0s 257us/step - loss: 0.5221 - accuracy: 0.7206 - val_loss: 0.4776 - val_accuracy: 0.8333\n",
      "Epoch 926/1000\n",
      "68/68 [==============================] - 0s 170us/step - loss: 0.5251 - accuracy: 0.7059 - val_loss: 0.4752 - val_accuracy: 0.8333\n",
      "Epoch 927/1000\n",
      "68/68 [==============================] - 0s 152us/step - loss: 0.5240 - accuracy: 0.6912 - val_loss: 0.4748 - val_accuracy: 0.8333\n",
      "Epoch 928/1000\n",
      "68/68 [==============================] - 0s 169us/step - loss: 0.5232 - accuracy: 0.6912 - val_loss: 0.4649 - val_accuracy: 0.8333\n",
      "Epoch 929/1000\n",
      "68/68 [==============================] - 0s 174us/step - loss: 0.5247 - accuracy: 0.6912 - val_loss: 0.4505 - val_accuracy: 0.8333\n",
      "Epoch 930/1000\n",
      "68/68 [==============================] - 0s 199us/step - loss: 0.5224 - accuracy: 0.7059 - val_loss: 0.4456 - val_accuracy: 0.8333\n",
      "Epoch 931/1000\n",
      "68/68 [==============================] - 0s 192us/step - loss: 0.5225 - accuracy: 0.7059 - val_loss: 0.4436 - val_accuracy: 0.8333\n",
      "Epoch 932/1000\n",
      "68/68 [==============================] - 0s 174us/step - loss: 0.5237 - accuracy: 0.7059 - val_loss: 0.4428 - val_accuracy: 0.8333\n",
      "Epoch 933/1000\n",
      "68/68 [==============================] - 0s 183us/step - loss: 0.5268 - accuracy: 0.7059 - val_loss: 0.4514 - val_accuracy: 0.8333\n",
      "Epoch 934/1000\n",
      "68/68 [==============================] - 0s 191us/step - loss: 0.5245 - accuracy: 0.7206 - val_loss: 0.5089 - val_accuracy: 0.7222\n",
      "Epoch 935/1000\n",
      "68/68 [==============================] - 0s 147us/step - loss: 0.5389 - accuracy: 0.7206 - val_loss: 0.5469 - val_accuracy: 0.6667\n",
      "Epoch 936/1000\n",
      "68/68 [==============================] - 0s 133us/step - loss: 0.5449 - accuracy: 0.7206 - val_loss: 0.5301 - val_accuracy: 0.6667\n",
      "Epoch 937/1000\n",
      "68/68 [==============================] - 0s 118us/step - loss: 0.5351 - accuracy: 0.7353 - val_loss: 0.4989 - val_accuracy: 0.7778\n",
      "Epoch 938/1000\n",
      "68/68 [==============================] - 0s 150us/step - loss: 0.5247 - accuracy: 0.7647 - val_loss: 0.4749 - val_accuracy: 0.7778\n",
      "Epoch 939/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5293 - accuracy: 0.7500 - val_loss: 0.4683 - val_accuracy: 0.7778\n",
      "Epoch 940/1000\n",
      "68/68 [==============================] - 0s 128us/step - loss: 0.5301 - accuracy: 0.7353 - val_loss: 0.4645 - val_accuracy: 0.8333\n",
      "Epoch 941/1000\n",
      "68/68 [==============================] - 0s 202us/step - loss: 0.5272 - accuracy: 0.7794 - val_loss: 0.4449 - val_accuracy: 0.8333\n",
      "Epoch 942/1000\n",
      "68/68 [==============================] - 0s 170us/step - loss: 0.5325 - accuracy: 0.7500 - val_loss: 0.4404 - val_accuracy: 0.8333\n",
      "Epoch 943/1000\n",
      "68/68 [==============================] - 0s 362us/step - loss: 0.5266 - accuracy: 0.7206 - val_loss: 0.4541 - val_accuracy: 0.8333\n",
      "Epoch 944/1000\n",
      "68/68 [==============================] - 0s 260us/step - loss: 0.5190 - accuracy: 0.7206 - val_loss: 0.4738 - val_accuracy: 0.7778\n",
      "Epoch 945/1000\n",
      "68/68 [==============================] - 0s 251us/step - loss: 0.5207 - accuracy: 0.7206 - val_loss: 0.4932 - val_accuracy: 0.7778\n",
      "Epoch 946/1000\n",
      "68/68 [==============================] - 0s 234us/step - loss: 0.5239 - accuracy: 0.7206 - val_loss: 0.4960 - val_accuracy: 0.7778\n",
      "Epoch 947/1000\n",
      "68/68 [==============================] - 0s 256us/step - loss: 0.5239 - accuracy: 0.7206 - val_loss: 0.4847 - val_accuracy: 0.7778\n",
      "Epoch 948/1000\n",
      "68/68 [==============================] - 0s 249us/step - loss: 0.5209 - accuracy: 0.7353 - val_loss: 0.4592 - val_accuracy: 0.7778\n",
      "Epoch 949/1000\n",
      "68/68 [==============================] - 0s 237us/step - loss: 0.5189 - accuracy: 0.7206 - val_loss: 0.4242 - val_accuracy: 0.8333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 950/1000\n",
      "68/68 [==============================] - 0s 309us/step - loss: 0.5638 - accuracy: 0.7206 - val_loss: 0.4186 - val_accuracy: 0.8333\n",
      "Epoch 951/1000\n",
      "68/68 [==============================] - 0s 259us/step - loss: 0.5768 - accuracy: 0.7059 - val_loss: 0.4259 - val_accuracy: 0.8333\n",
      "Epoch 952/1000\n",
      "68/68 [==============================] - 0s 239us/step - loss: 0.5329 - accuracy: 0.7059 - val_loss: 0.4581 - val_accuracy: 0.7778\n",
      "Epoch 953/1000\n",
      "68/68 [==============================] - 0s 341us/step - loss: 0.5196 - accuracy: 0.7206 - val_loss: 0.4978 - val_accuracy: 0.7778\n",
      "Epoch 954/1000\n",
      "68/68 [==============================] - 0s 140us/step - loss: 0.5368 - accuracy: 0.7059 - val_loss: 0.5216 - val_accuracy: 0.7222\n",
      "Epoch 955/1000\n",
      "68/68 [==============================] - 0s 251us/step - loss: 0.5402 - accuracy: 0.7059 - val_loss: 0.4964 - val_accuracy: 0.7222\n",
      "Epoch 956/1000\n",
      "68/68 [==============================] - 0s 367us/step - loss: 0.5312 - accuracy: 0.6912 - val_loss: 0.4501 - val_accuracy: 0.8333\n",
      "Epoch 957/1000\n",
      "68/68 [==============================] - 0s 170us/step - loss: 0.5289 - accuracy: 0.7059 - val_loss: 0.4227 - val_accuracy: 0.7778\n",
      "Epoch 958/1000\n",
      "68/68 [==============================] - 0s 362us/step - loss: 0.5467 - accuracy: 0.7206 - val_loss: 0.4205 - val_accuracy: 0.7778\n",
      "Epoch 959/1000\n",
      "68/68 [==============================] - 0s 260us/step - loss: 0.5587 - accuracy: 0.7353 - val_loss: 0.4233 - val_accuracy: 0.7778\n",
      "Epoch 960/1000\n",
      "68/68 [==============================] - 0s 354us/step - loss: 0.5450 - accuracy: 0.7353 - val_loss: 0.4379 - val_accuracy: 0.8333\n",
      "Epoch 961/1000\n",
      "68/68 [==============================] - 0s 224us/step - loss: 0.5268 - accuracy: 0.7059 - val_loss: 0.4624 - val_accuracy: 0.8333\n",
      "Epoch 962/1000\n",
      "68/68 [==============================] - 0s 244us/step - loss: 0.5301 - accuracy: 0.7059 - val_loss: 0.4842 - val_accuracy: 0.7778\n",
      "Epoch 963/1000\n",
      "68/68 [==============================] - 0s 296us/step - loss: 0.5240 - accuracy: 0.7206 - val_loss: 0.4826 - val_accuracy: 0.7778\n",
      "Epoch 964/1000\n",
      "68/68 [==============================] - 0s 302us/step - loss: 0.5224 - accuracy: 0.7647 - val_loss: 0.4686 - val_accuracy: 0.8333\n",
      "Epoch 965/1000\n",
      "68/68 [==============================] - 0s 156us/step - loss: 0.5243 - accuracy: 0.7794 - val_loss: 0.4510 - val_accuracy: 0.8333\n",
      "Epoch 966/1000\n",
      "68/68 [==============================] - 0s 174us/step - loss: 0.5495 - accuracy: 0.7500 - val_loss: 0.4416 - val_accuracy: 0.8333\n",
      "Epoch 967/1000\n",
      "68/68 [==============================] - 0s 221us/step - loss: 0.5832 - accuracy: 0.7500 - val_loss: 0.4372 - val_accuracy: 0.8333\n",
      "Epoch 968/1000\n",
      "68/68 [==============================] - 0s 323us/step - loss: 0.5682 - accuracy: 0.7647 - val_loss: 0.4542 - val_accuracy: 0.8333\n",
      "Epoch 969/1000\n",
      "68/68 [==============================] - 0s 324us/step - loss: 0.5266 - accuracy: 0.7059 - val_loss: 0.5102 - val_accuracy: 0.7222\n",
      "Epoch 970/1000\n",
      "68/68 [==============================] - 0s 50us/step - loss: 0.5332 - accuracy: 0.7206 - val_loss: 0.5701 - val_accuracy: 0.6111\n",
      "Epoch 971/1000\n",
      "68/68 [==============================] - 0s 230us/step - loss: 0.5633 - accuracy: 0.7059 - val_loss: 0.5726 - val_accuracy: 0.6111\n",
      "Epoch 972/1000\n",
      "68/68 [==============================] - ETA: 0s - loss: 0.5647 - accuracy: 0.71 - 0s 416us/step - loss: 0.5527 - accuracy: 0.7059 - val_loss: 0.5161 - val_accuracy: 0.7222\n",
      "Epoch 973/1000\n",
      "68/68 [==============================] - 0s 148us/step - loss: 0.5300 - accuracy: 0.7206 - val_loss: 0.4642 - val_accuracy: 0.8333\n",
      "Epoch 974/1000\n",
      "68/68 [==============================] - 0s 342us/step - loss: 0.5159 - accuracy: 0.7059 - val_loss: 0.4401 - val_accuracy: 0.8333\n",
      "Epoch 975/1000\n",
      "68/68 [==============================] - 0s 152us/step - loss: 0.5204 - accuracy: 0.7206 - val_loss: 0.4350 - val_accuracy: 0.8333\n",
      "Epoch 976/1000\n",
      "68/68 [==============================] - 0s 230us/step - loss: 0.5209 - accuracy: 0.7059 - val_loss: 0.4438 - val_accuracy: 0.8333\n",
      "Epoch 977/1000\n",
      "68/68 [==============================] - 0s 230us/step - loss: 0.5174 - accuracy: 0.7059 - val_loss: 0.4577 - val_accuracy: 0.8333\n",
      "Epoch 978/1000\n",
      "68/68 [==============================] - 0s 230us/step - loss: 0.5220 - accuracy: 0.7059 - val_loss: 0.4673 - val_accuracy: 0.7778\n",
      "Epoch 979/1000\n",
      "68/68 [==============================] - 0s 230us/step - loss: 0.5185 - accuracy: 0.7059 - val_loss: 0.4652 - val_accuracy: 0.7778\n",
      "Epoch 980/1000\n",
      "68/68 [==============================] - 0s 230us/step - loss: 0.5160 - accuracy: 0.7353 - val_loss: 0.4660 - val_accuracy: 0.7778\n",
      "Epoch 981/1000\n",
      "68/68 [==============================] - 0s 230us/step - loss: 0.5159 - accuracy: 0.7353 - val_loss: 0.4744 - val_accuracy: 0.7778\n",
      "Epoch 982/1000\n",
      "68/68 [==============================] - 0s 230us/step - loss: 0.5234 - accuracy: 0.7353 - val_loss: 0.5131 - val_accuracy: 0.7222\n",
      "Epoch 983/1000\n",
      "68/68 [==============================] - 0s 0us/step - loss: 0.5292 - accuracy: 0.7206 - val_loss: 0.5193 - val_accuracy: 0.7222\n",
      "Epoch 984/1000\n",
      "68/68 [==============================] - 0s 187us/step - loss: 0.5267 - accuracy: 0.7206 - val_loss: 0.4853 - val_accuracy: 0.7778\n",
      "Epoch 985/1000\n",
      "68/68 [==============================] - 0s 259us/step - loss: 0.5221 - accuracy: 0.7794 - val_loss: 0.4574 - val_accuracy: 0.7778\n",
      "Epoch 986/1000\n",
      "68/68 [==============================] - 0s 159us/step - loss: 0.5211 - accuracy: 0.7794 - val_loss: 0.4508 - val_accuracy: 0.8333\n",
      "Epoch 987/1000\n",
      "68/68 [==============================] - 0s 209us/step - loss: 0.5198 - accuracy: 0.7794 - val_loss: 0.4574 - val_accuracy: 0.7778\n",
      "Epoch 988/1000\n",
      "68/68 [==============================] - 0s 267us/step - loss: 0.5139 - accuracy: 0.7353 - val_loss: 0.4656 - val_accuracy: 0.7778\n",
      "Epoch 989/1000\n",
      "68/68 [==============================] - 0s 230us/step - loss: 0.5147 - accuracy: 0.7353 - val_loss: 0.4503 - val_accuracy: 0.8333\n",
      "Epoch 990/1000\n",
      "68/68 [==============================] - 0s 329us/step - loss: 0.5142 - accuracy: 0.7206 - val_loss: 0.4445 - val_accuracy: 0.8333\n",
      "Epoch 991/1000\n",
      "68/68 [==============================] - 0s 136us/step - loss: 0.5142 - accuracy: 0.7353 - val_loss: 0.4449 - val_accuracy: 0.7778\n",
      "Epoch 992/1000\n",
      "68/68 [==============================] - 0s 273us/step - loss: 0.5160 - accuracy: 0.7353 - val_loss: 0.4512 - val_accuracy: 0.7778\n",
      "Epoch 993/1000\n",
      "68/68 [==============================] - 0s 230us/step - loss: 0.5164 - accuracy: 0.7500 - val_loss: 0.4586 - val_accuracy: 0.7778\n",
      "Epoch 994/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5163 - accuracy: 0.7353 - val_loss: 0.4569 - val_accuracy: 0.7778\n",
      "Epoch 995/1000\n",
      "68/68 [==============================] - 0s 176us/step - loss: 0.5143 - accuracy: 0.7206 - val_loss: 0.4403 - val_accuracy: 0.8333\n",
      "Epoch 996/1000\n",
      "68/68 [==============================] - 0s 162us/step - loss: 0.5157 - accuracy: 0.7206 - val_loss: 0.4320 - val_accuracy: 0.8333\n",
      "Epoch 997/1000\n",
      "68/68 [==============================] - 0s 174us/step - loss: 0.5177 - accuracy: 0.7206 - val_loss: 0.4371 - val_accuracy: 0.8333\n",
      "Epoch 998/1000\n",
      "68/68 [==============================] - 0s 115us/step - loss: 0.5125 - accuracy: 0.7206 - val_loss: 0.4527 - val_accuracy: 0.8333\n",
      "Epoch 999/1000\n",
      "68/68 [==============================] - 0s 290us/step - loss: 0.5118 - accuracy: 0.7059 - val_loss: 0.4703 - val_accuracy: 0.7778\n",
      "Epoch 1000/1000\n",
      "68/68 [==============================] - 0s 109us/step - loss: 0.5161 - accuracy: 0.7059 - val_loss: 0.4744 - val_accuracy: 0.7778\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(train_data,new_train_target,epochs=1000,validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25c12ce2dd8>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25c12eb7f60>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'],'r')\n",
    "plt.plot(history.history['val_accuracy'],'g')\n",
    "plt.xlabel('# epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 200us/step\n",
      "[0.6274320483207703, 0.699999988079071]\n"
     ]
    }
   ],
   "source": [
    "new_test_target = np_utils.to_categorical(test_target)\n",
    "\n",
    "print(model.evaluate(test_data , new_test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 36        \n",
      "=================================================================\n",
      "Total params: 316\n",
      "Trainable params: 316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('Mango_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.9851232e-01 3.0147800e-01 9.6846043e-06 6.2295719e-08]\n",
      " [6.8208778e-01 3.1789923e-01 1.2967862e-05 8.5417909e-08]\n",
      " [6.8492141e-06 3.4309787e-04 7.9607886e-01 2.0357114e-01]\n",
      " [7.3770773e-01 2.6228580e-01 6.4020360e-06 4.8299022e-08]\n",
      " [2.5237468e-06 1.5094361e-04 7.6097667e-01 2.3886988e-01]\n",
      " [6.1679447e-01 3.8314590e-01 5.9006561e-05 5.6812058e-07]\n",
      " [8.6488299e-06 3.3672122e-04 7.2449481e-01 2.7515984e-01]\n",
      " [3.3866294e-02 1.4986958e-01 6.3878161e-01 1.7748249e-01]\n",
      " [1.8125502e-07 1.7437809e-05 6.5798318e-01 3.4199911e-01]\n",
      " [5.8613233e-03 6.9855846e-02 8.4293538e-01 8.1347398e-02]]\n",
      "[[2 0 0 0]\n",
      " [2 0 0 0]\n",
      " [0 0 5 0]\n",
      " [0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "predicted_target=model.predict(test_data)\n",
    "print(predicted_target)\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "#https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781838555078/6/ch06lvl1sec34/confusion-matrix\n",
    "#cm=confusion_matrix(y_test,y_pred_class)\n",
    "cm=confusion_matrix(np.argmax(new_test_target,axis=1),np.argmax(predicted_target,axis=1))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
